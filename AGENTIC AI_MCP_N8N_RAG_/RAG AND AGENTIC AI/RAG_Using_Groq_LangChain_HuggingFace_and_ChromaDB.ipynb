{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG-Using-Groq-LangChain-HuggingFace-and-ChromaDB**"
      ],
      "metadata": {
        "id": "HC4qnLVo798F"
      },
      "id": "HC4qnLVo798F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Pipeline Overview\n",
        "\n",
        "This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using **LangChain**, **open-source Hugging Face embeddings**, **Chroma vector database**, and a **Groq-hosted LLM** for high-speed inference. The pipeline starts by loading a raw text document and splitting it into semantically meaningful chunks using a recursive text splitter. Each chunk is then converted into dense vector embeddings using a Sentence-Transformers model and stored in a Chroma vector store for efficient similarity-based retrieval.\n",
        "\n",
        "At query time, the user’s question is embedded and compared against the stored vectors to retrieve the most relevant document chunks. These retrieved chunks are provided as contextual input to the LLM, which generates an answer grounded strictly in the source content. The system also returns the source documents used for generation, ensuring transparency and explainability. This architecture keeps retrieval fully open-source, decouples knowledge storage from generation, and enables scalable, accurate question-answering over custom text data.\n"
      ],
      "metadata": {
        "id": "9d2tQzH05ekp"
      },
      "id": "9d2tQzH05ekp"
    },
    {
      "cell_type": "markdown",
      "id": "dc160f60",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "papermill": {
          "duration": 0.009055,
          "end_time": "2023-10-27T20:56:19.591480",
          "exception": false,
          "start_time": "2023-10-27T20:56:19.582425",
          "status": "completed"
        },
        "tags": [],
        "id": "dc160f60"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "## Objective\n",
        "\n",
        "Build a **Retrieval-Augmented Generation (RAG)** based question–answering system using **LangChain**, **open-source Hugging Face embeddings**, **ChromaDB** as the vector database, and a **Groq-hosted LLM** for fast inference.  \n",
        "The goal is to ask natural language questions over a custom text file (`biden-sotu-2023-planned-official.txt`) without fine-tuning any Large Language Model.\n",
        "\n",
        "In a RAG setup, when a user asks a question, the system first retrieves the most relevant document chunks from a vector database and then uses an LLM to generate an answer grounded in those retrieved documents.\n",
        "\n",
        "---\n",
        "\n",
        "## Definitions\n",
        "\n",
        "* **LLM (Large Language Model)** – A neural network trained on massive text corpora to understand and generate human-like language  \n",
        "* **LangChain** – A framework for building applications powered by LLMs, especially retrieval-based pipelines  \n",
        "* **Text Embeddings** – Dense vector representations of text capturing semantic meaning  \n",
        "* **Vector Database** – A database optimized for storing and searching high-dimensional vectors  \n",
        "* **ChromaDB** – An open-source vector database used for embedding storage and similarity search  \n",
        "* **RAG (Retrieval-Augmented Generation)** – A technique that combines information retrieval with text generation  \n",
        "\n",
        "---\n",
        "\n",
        "## Model & Technology Stack\n",
        "\n",
        "* **LLM Provider**: Groq  \n",
        "* **LLM Model**: `llama-3.1-8b-instant`  \n",
        "* **Embedding Model**: Hugging Face Sentence-Transformers (open source)  \n",
        "* **Vector Store**: ChromaDB (local, persistent)  \n",
        "* **Framework**: LangChain  \n",
        "* **Environment**: Google Colab  \n",
        "\n",
        "This setup ensures that **retrieval and storage remain fully open-source**, while Groq is used only for fast, cost-effective LLM inference.\n",
        "\n",
        "---\n",
        "\n",
        "## What is a Retrieval-Augmented Generation (RAG) System?\n",
        "\n",
        "Large Language Models are highly capable when answering questions based on knowledge seen during training. However, they can hallucinate or produce incorrect answers when queried about private, recent, or domain-specific data they were never trained on.\n",
        "\n",
        "A **RAG system** addresses this limitation by augmenting the LLM with an external knowledge source.\n",
        "\n",
        "A RAG pipeline has two core components:\n",
        "\n",
        "### 1. Retriever  \n",
        "The retriever converts documents into embeddings and stores them in a vector database.  \n",
        "At query time, the user’s question is embedded and compared against stored vectors to retrieve the most relevant document chunks.\n",
        "\n",
        "In this project:\n",
        "- Text is split into chunks using a recursive text splitter  \n",
        "- Embeddings are generated using an open-source Hugging Face model  \n",
        "- ChromaDB is used for similarity-based retrieval  \n",
        "\n",
        "### 2. Generator  \n",
        "The generator is an LLM that takes:\n",
        "- The user query  \n",
        "- The retrieved document chunks  \n",
        "\n",
        "and produces a grounded, context-aware answer.\n",
        "\n",
        "Here, a **Groq-hosted LLaMA 3.1 Instant model** is used for generation, ensuring fast responses while relying on retrieved content for factual accuracy.\n",
        "\n",
        "LangChain orchestrates this entire flow, allowing retrieval and generation to be combined into a single, clean pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Architecture?\n",
        "\n",
        "* Avoids LLM hallucination by grounding answers in source documents  \n",
        "* No fine-tuning required  \n",
        "* Works with private or custom data  \n",
        "* Modular and extensible  \n",
        "* Open-source embeddings and vector storage  \n",
        "* Production-ready RAG pattern used in real-world systems  \n",
        "\n",
        "---\n",
        "\n",
        "## More About This\n",
        "\n",
        "This notebook demonstrates a practical, end-to-end RAG workflow suitable for research, enterprise QA bots, and document intelligence systems.  \n",
        "You can extend this pipeline with:\n",
        "- Multiple documents  \n",
        "- Metadata-based filtering  \n",
        "- MLflow/DagsHub experiment tracking  \n",
        "- Evaluation and feedback loops  \n",
        "\n",
        "Refer to the **References** section for deeper reading on LangChain, ChromaDB, embeddings, and RAG system design.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vd5XNBpP69Io"
      },
      "id": "vd5XNBpP69Io",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Pipeline Overview\n",
        "\n",
        "This section outlines the complete project pipeline, broken down step by step, as implemented in the notebook to build a Retrieval-Augmented Generation (RAG) system.\n",
        "\n",
        "### Introduction and Objective  \n",
        "The project aims to build a **RAG-based question–answering system** using **LangChain**, **open-source Hugging Face embeddings**, **ChromaDB**, and a **Groq-hosted LLM** for fast inference.  \n",
        "The system enables natural language querying over a custom text file (`biden-sotu-2023-planned-official.txt`) without fine-tuning any large language model.\n",
        "\n",
        "### Environment Setup and Library Installation  \n",
        "All required Python libraries are installed, including:\n",
        "- `langchain`\n",
        "- `langchain-community`\n",
        "- `langchain-text-splitters`\n",
        "- `chromadb`\n",
        "- `sentence-transformers`\n",
        "- `groq`\n",
        "- `langchain-groq`  \n",
        "\n",
        "These libraries collectively support document loading, text splitting, embeddings, vector storage, retrieval, and LLM-based generation.\n",
        "\n",
        "### Imports & Environment Validation  \n",
        "Necessary modules are imported, and the **Groq API key** is retrieved securely from Colab secrets (`GROQ_API_KEY`).  \n",
        "The environment is validated to ensure the key is accessible and the Groq client can be initialized successfully.\n",
        "\n",
        "### Load TXT Knowledge Base  \n",
        "The custom knowledge source (`biden-sotu-2023-planned-official.txt`) is loaded into memory.  \n",
        "This document serves as the external knowledge base over which the RAG system will operate.\n",
        "\n",
        "### Chunk the Document  \n",
        "The loaded text is split into smaller, semantically meaningful chunks using a **RecursiveCharacterTextSplitter**.  \n",
        "Chunking helps manage context length limits and improves the quality of embedding-based retrieval.\n",
        "\n",
        "### Create Embeddings  \n",
        "A **Hugging Face embedding model** (e.g., `sentence-transformers/all-MiniLM-L6-v2`) is initialized to convert each text chunk into a dense vector representation that captures semantic meaning.\n",
        "\n",
        "### Create Chroma Vector Database  \n",
        "A **local, persistent ChromaDB vector store** is created using the text chunks and their embeddings.  \n",
        "This vector database enables efficient similarity search during query time.\n",
        "\n",
        "### Initialize Groq LLM  \n",
        "The **ChatGroq** Large Language Model is initialized with:\n",
        "- Model name (e.g., `llama-3.1-8b-instant`)\n",
        "- Groq API key\n",
        "- Temperature parameter for controlled response generation  \n",
        "\n",
        "This LLM is responsible for generating final answers based on retrieved context.\n",
        "\n",
        "### Build the RAG Pipeline  \n",
        "The complete RAG pipeline is constructed using **LangChain’s `RetrievalQA`** chain.  \n",
        "This step combines:\n",
        "- The **ChromaDB retriever** for fetching relevant document chunks  \n",
        "- The **Groq LLM** for generating grounded, context-aware answers  \n",
        "\n",
        "### Ask Questions  \n",
        "Users can now interact with the system by asking natural language questions.  \n",
        "The pipeline retrieves relevant document sections and generates answers, optionally returning the source documents used for retrieval to ensure transparency and trust.\n"
      ],
      "metadata": {
        "id": "Gvez4Ucp7swt"
      },
      "id": "Gvez4Ucp7swt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create the Environment and Install Libraries**"
      ],
      "metadata": {
        "id": "ZU5lrzc-wM34"
      },
      "id": "ZU5lrzc-wM34"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --q -U langchain langchain-community langchain-core"
      ],
      "metadata": {
        "id": "k-0TDFrM1xWp"
      },
      "id": "k-0TDFrM1xWp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all installed packages that contain the word \"langchain\"\n",
        "!pip list | grep langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T-FIO0218en",
        "outputId": "22df22f6-7271-46a3-8ee6-0ecca885d158"
      },
      "id": "7T-FIO0218en",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain                                1.2.4\n",
            "langchain-classic                        1.0.1\n",
            "langchain-community                      0.4.1\n",
            "langchain-core                           1.2.6\n",
            "langchain-groq                           1.1.1\n",
            "langchain-text-splitters                 1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Install required Python packages for building a RAG (Retrieval-Augmented Generation) pipeline\n",
        "  # using LangChain with ChromaDB vector store and Groq inference\n",
        "  # The -U flag upgrades packages if already installed\n",
        "\n",
        "!pip install -U \\\n",
        "langchain \\\n",
        "langchain-community \\\n",
        "langchain-text-splitters \\\n",
        "chromadb \\\n",
        "sentence-transformers \\\n",
        "groq \\\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD-6cuT5wNKr",
        "outputId": "ae786f8e-dc34-494e-a6b4-7f3989b15aff"
      },
      "id": "zD-6cuT5wNKr",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.2.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.4.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting groq\n",
            "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.6)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading pypika-0.50.0-py2.py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.26.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain-1.2.4-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading chromadb-1.4.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.4.0-py3-none-any.whl (24 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypika-0.50.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: pypika, durationpy, urllib3, pyproject_hooks, pybase64, opentelemetry-proto, mypy-extensions, marshmallow, humanfriendly, bcrypt, backoff, typing-inspect, requests, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, posthog, opentelemetry-semantic-conventions, onnxruntime, groq, dataclasses-json, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, chromadb, langchain-classic, langchain-community, langchain\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.2.3\n",
            "    Uninstalling langchain-1.2.3:\n",
            "      Successfully uninstalled langchain-1.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.4.0 chromadb-1.4.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 groq-1.0.0 humanfriendly-10.0 kubernetes-34.1.0 langchain-1.2.4 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-text-splitters-1.1.0 marshmallow-3.26.2 mypy-extensions-1.1.0 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pypika-0.50.0 pyproject_hooks-1.2.0 requests-2.32.5 typing-inspect-0.9.0 urllib3-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --q langchain-groq"
      ],
      "metadata": {
        "id": "jBEplO_gyIAD"
      },
      "id": "jBEplO_gyIAD",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PHASE 1 — Imports & environment validation**"
      ],
      "metadata": {
        "id": "B8b8UDZfwM1O"
      },
      "id": "B8b8UDZfwM1O"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules for building a RAG (Retrieval-Augmented Generation) pipeline\n",
        "# using LangChain with HuggingFace embeddings, Chroma vector store, and Groq LLM\n",
        "\n",
        "import os  # Standard library for environment variables and file paths\n",
        "\n",
        "# Text splitting utilities for chunking documents into semantically coherent pieces\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Local embeddings using HuggingFace sentence-transformers models (offline-capable)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# ChromaDB vector database integration - persistent storage for document embeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Groq API client wrapper for high-speed LLM inference (Mixtral/Llama3 models)\n",
        "from langchain_groq.chat_models import ChatGroq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeqKJfpwwzej",
        "outputId": "a16166ad-6e40-4548-ce9e-4b0e4e8db87d"
      },
      "id": "yeqKJfpwwzej",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validate Groq API key:**"
      ],
      "metadata": {
        "id": "oznUjjzfxNIS"
      },
      "id": "oznUjjzfxNIS"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_key = userdata.get(\"GROQ_API_KEY\")\n",
        "assert groq_key is not None, \"GROQ_API_KEY not found in Colab secrets\""
      ],
      "metadata": {
        "id": "KpCV0R9XxKOr"
      },
      "id": "KpCV0R9XxKOr",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.2,\n",
        "    groq_api_key=groq_key\n",
        ")"
      ],
      "metadata": {
        "id": "NkhXN2l13T4p"
      },
      "id": "NkhXN2l13T4p",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Say hello in one sentence.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SDf1PdP3KxL",
        "outputId": "861a6f38-6a44-4835-9b98-191195ab29a8"
      },
      "id": "-SDf1PdP3KxL",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello, how can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 41, 'total_tokens': 51, 'completion_time': 0.010869875, 'completion_tokens_details': None, 'prompt_time': 0.002017812, 'prompt_tokens_details': None, 'queue_time': 0.005650345, 'total_time': 0.012887687}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bc0dd-ec33-71c2-954a-174c856fdfd2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 41, 'output_tokens': 10, 'total_tokens': 51})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PHASE 2 — Load your TXT knowledge base**"
      ],
      "metadata": {
        "id": "CA-nbMD3yUbw"
      },
      "id": "CA-nbMD3yUbw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to the input text file containing Biden's 2023 State of the Union address\n",
        "# /content/ path indicates Google Colab environment with mounted Drive/dataset\n",
        "file_path = \"/content/biden-sotu-2023-planned-official.txt\"\n",
        "\n",
        "# Open file in read mode with UTF-8 encoding to handle special characters/quotes\n",
        "# Context manager (with statement) ensures automatic file closure even if errors occur\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()  # Read entire file content into string variable\n",
        "\n",
        "# Display document statistics - character count helps assess text splitter chunk size needs\n",
        "print(\"Document length (characters):\", len(raw_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uomQUJK0xnIT",
        "outputId": "e1f58d96-38d1-41da-f075-f562d4bde0be"
      },
      "id": "uomQUJK0xnIT",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document length (characters): 41661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PHASE 3 — Chunk the document**"
      ],
      "metadata": {
        "id": "9IeNuu3UzEjE"
      },
      "id": "9IeNuu3UzEjE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize text splitter for RAG pipeline with optimal chunking parameters\n",
        "# chunk_size=500: Creates ~500 char chunks (fits most embedding models' context)\n",
        "# chunk_overlap=100: 20% overlap maintains context across chunk boundaries\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,      # Maximum characters per chunk\n",
        "    chunk_overlap=100    # Overlapping characters between consecutive chunks\n",
        ")\n",
        "\n",
        "# Split raw SOTU document into semantically coherent text chunks\n",
        "# Recursive splitter tries: paragraphs → sentences → words → characters\n",
        "chunks = text_splitter.split_text(raw_text)\n",
        "\n",
        "# Verify chunking results and preview formatting\n",
        "print(\"Total chunks:\", len(chunks))\n",
        "print(\"Sample chunk:\\n\", chunks[0][:300])  # First 300 chars of first chunk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8pGTEvdzHsj",
        "outputId": "e81b3bfb-d275-4ced-9ad4-d23a93b5d361"
      },
      "id": "E8pGTEvdzHsj",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 105\n",
            "Sample chunk:\n",
            " Mr. Speaker. Madam Vice President. Our First Lady and Second Gentleman. Members of Congress and the Cabinet. Leaders of our military. Mr. Chief Justice, Associate Justices, and retired Justices of the Supreme Court. And you, my fellow Americans. I start tonight by congratulating the members of the 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PHASE 4 — Create embeddings**"
      ],
      "metadata": {
        "id": "-Mno_to_z-LQ"
      },
      "id": "-Mno_to_z-LQ"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RIVB0Vi0TBQ",
        "outputId": "8c07e7ec-381c-4668-9047-55b4fe49f40a"
      },
      "id": "9RIVB0Vi0TBQ",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3825674162.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PHASE 5 — Create Chroma vector database (local, open source)**"
      ],
      "metadata": {
        "id": "b4OyLxtQ0YdE"
      },
      "id": "b4OyLxtQ0YdE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create persistent Chroma vector database from text chunks for RAG semantic search\n",
        "# Automatically generates embeddings for each chunk using the HuggingFace model\n",
        "vectordb = Chroma.from_texts(\n",
        "    texts=chunks,                    # List of pre-split text chunks from SOTU document\n",
        "    embedding=embedding_model,       # Pre-initialized HuggingFaceEmbeddings instance\n",
        "    persist_directory=\"/content/chroma_db\"  # Local directory for persistent storage\n",
        ")\n",
        "\n",
        "# Convert vector database into retriever component for LangChain chains\n",
        "# k=3 retrieves top 3 most semantically similar chunks per query\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n"
      ],
      "metadata": {
        "id": "Xv2mFnya0cTX"
      },
      "id": "Xv2mFnya0cTX",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PHASE 6 — Initialize Groq LLM**"
      ],
      "metadata": {
        "id": "xV7HAq9D0l7y"
      },
      "id": "xV7HAq9D0l7y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Groq LLM client for fast inference in RAG pipeline\n",
        "# llama-3.1-8b-instant: Meta's 8B parameter model optimized for Groq LPUs (300+ tokens/sec)\n",
        "llm = ChatGroq(\n",
        "    model_name=\"llama-3.1-8b-instant\",  # Fastest Groq model for real-time RAG responses\n",
        "    groq_api_key=groq_key,              # API key from environment variable or direct input\n",
        "    temperature=0.2                     # Low temperature for factual, consistent responses\n",
        ")\n"
      ],
      "metadata": {
        "id": "UlHARhvz0rCB"
      },
      "id": "UlHARhvz0rCB",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PHASE 7 — Build the RAG pipeline (Retriever + LLM)**"
      ],
      "metadata": {
        "id": "-htaqIEU00-5"
      },
      "id": "-htaqIEU00-5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Change from: from langchain.chains import RetrievalQA\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "\n",
        "# Create complete RAG pipeline combining retriever + LLM\n",
        "# Automatically handles: embed query → retrieve chunks → generate answer\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,                           # Groq Llama 3.1 model for answer generation\n",
        "    retriever=retriever,               # ChromaDB top-3 chunk retriever\n",
        "    chain_type=\"stuff\",                # Stuff all retrieved docs into single LLM prompt\n",
        "    return_source_documents=True       # Include source chunks with every answer\n",
        ")\n"
      ],
      "metadata": {
        "id": "XbBpkFkl03QI"
      },
      "id": "XbBpkFkl03QI",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PHASE 8 — Ask questions (RAG in action)**"
      ],
      "metadata": {
        "id": "ixJJSYXe2PNB"
      },
      "id": "ixJJSYXe2PNB"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What were the main goals of the Biden administration mentioned in the speech?\"\n",
        "\n",
        "result = qa_chain.invoke(query)\n",
        "\n",
        "print(\"Answer:\\n\", result[\"result\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyV5vqAz2RaJ",
        "outputId": "2b31fe07-9fac-4467-9822-b3ce214d626e"
      },
      "id": "HyV5vqAz2RaJ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            " The main goals of the Biden administration mentioned in the speech are:\n",
            "\n",
            "1. To restore the soul of the nation.\n",
            "2. To rebuild the backbone of America, specifically the middle class.\n",
            "3. To unite the country.\n",
            "\n",
            "These goals are mentioned as part of the administration's vision for the country, and the speech suggests that the administration has been working to achieve these goals through bipartisan efforts in Congress.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSources used:\")\n",
        "for doc in result[\"source_documents\"]:\n",
        "    print(\"-\", doc.page_content[:200], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziyD4KN52Yiy",
        "outputId": "791cf6a1-22d4-4822-8417-0589ecf23ec3"
      },
      "id": "ziyD4KN52Yiy",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sources used:\n",
            "- work together in this new Congress. The people sent us a clear message. Fighting for the sake of fighting, power for the sake of power, conflict for the sake of conflict, gets us nowhere. And that’s a ...\n",
            "- work together in this new Congress. The people sent us a clear message. Fighting for the sake of fighting, power for the sake of power, conflict for the sake of conflict, gets us nowhere. And that’s a ...\n",
            "- America. When I came to office, most everyone assumed bipartisanship was impossible. But I never believed it. That’s why a year ago, I offered a Unity Agenda for the nation. We’ve made real progress.  ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain.invoke({\"query\": query})\n",
        "print(result[\"result\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgsyo-Z04HuJ",
        "outputId": "06e3f4e7-c0b2-4a0f-81e0-eb185b22419f"
      },
      "id": "tgsyo-Z04HuJ",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main goals of the Biden administration mentioned in the speech are:\n",
            "\n",
            "1. To restore the soul of the nation.\n",
            "2. To rebuild the backbone of America, specifically the middle class.\n",
            "3. To unite the country.\n",
            "\n",
            "Additionally, the speech mentions specific actions taken by the administration, such as:\n",
            "\n",
            "1. Making it easier for doctors to prescribe effective treatments for opioid addiction.\n",
            "2. Passing a gun safety law making historic investments in mental health.\n",
            "3. Launching ARPA-H to drive breakthroughs in the fight against cancer, Alzheimer's, diabetes, and other diseases.\n",
            "\n",
            "These goals and actions are part of the administration's vision for the country and its efforts to \"finish the job\" that the people sent them to do.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Anaconda)",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 453.685483,
      "end_time": "2023-10-27T21:03:49.973763",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-10-27T20:56:16.288280",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}