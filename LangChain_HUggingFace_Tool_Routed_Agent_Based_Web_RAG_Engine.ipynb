{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7lsgBIjLUy_"
   },
   "source": [
    "# **LangChain_HUggingFace_Tool-Routed Agent Based Web RAG Engine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhHD8znC0lD5"
   },
   "source": [
    "# Overall Goal\n",
    "\n",
    "This pipeline is designed to answer user questions comprehensively by first consulting an internal knowledge base. If the internal retrieval is insufficient, the system intelligently expands its search to external specialized sources such as Wikipedia for general knowledge and Arxiv for academic literature. All retrieved information is then synthesized into a single, coherent, and well-cited final answer.\n",
    "\n",
    "# Core Components and Modular Functions\n",
    "\n",
    "## Document Loaders\n",
    "\n",
    "The system uses modular document loaders to ingest content from multiple sources, each tagged for traceability.\n",
    "\n",
    "- **load_pdf(file_path: str)**  \n",
    "  Reads and processes content from local files such as `/content/LLM.pdf`. The extracted text is tagged as `[PDF]` and handled robustly to support standard text-based documents.\n",
    "\n",
    "- **load_web_docs(url: str)**  \n",
    "  Fetches and parses content from a specified web URL, such as `https://docs.smith.langchain.com/`. All extracted text is tagged as `[Web]`.\n",
    "\n",
    "- **load_wikipedia(query: str)**  \n",
    "  Executes a Wikipedia search based on the query and tags the retrieved content as `[Wikipedia]`.\n",
    "\n",
    "- **load_arxiv(query: str)**  \n",
    "  Searches Arxiv for relevant academic papers and tags the retrieved content as `[Arxiv]`.\n",
    "\n",
    "## Document Processing\n",
    "\n",
    "- **RecursiveCharacterTextSplitter**  \n",
    "  All loaded documents are segmented into smaller, semantically meaningful chunks to improve retrieval accuracy and embedding quality.\n",
    "\n",
    "## Vector Store and Retrieval\n",
    "\n",
    "- **Embeddings**  \n",
    "  HuggingFaceEmbeddings using the `BAAI/bge-small-en-v1.5` model are used to generate dense vector representations for each document chunk.\n",
    "\n",
    "- **Vector Store**  \n",
    "  All embeddings are stored in a FAISS vector database.\n",
    "\n",
    "- **Retriever**  \n",
    "  A retriever queries the FAISS index to return the top `k` most relevant document chunks (for example, `k = 3`) for a given user question.\n",
    "\n",
    "## Document Formatting\n",
    "\n",
    "- **format_docs(docs: List[Document])**  \n",
    "  A helper function that formats retrieved document chunks into a structured text block, preserving inline source tags such as `[PDF]`, `[Web]`, `[Wikipedia]`, and `[Arxiv]`.\n",
    "\n",
    "## Language Model\n",
    "\n",
    "- **LLM**  \n",
    "  `ChatGroq` with the model `llama-3.1-8b-instant` serves as the primary language model. It is responsible for routing decisions, intermediate reasoning, and final answer generation.\n",
    "\n",
    "## Tools\n",
    "\n",
    "The system exposes tool functions using decorators, enabling the agent to dynamically invoke them during execution.\n",
    "\n",
    "- **internal_knowledge_base**  \n",
    "  Retrieves and formats relevant content from the FAISS vector store.\n",
    "\n",
    "- **wikipedia_search**  \n",
    "  Queries Wikipedia using the Wikipedia loader and returns formatted results.\n",
    "\n",
    "- **arxiv_search**  \n",
    "  Queries Arxiv using the Arxiv loader and returns formatted academic content.\n",
    "\n",
    "## Orchestration with LangGraph\n",
    "\n",
    "LangGraph manages the entire workflow using a state-machine-based execution model.\n",
    "\n",
    "# Step-by-Step Pipeline Flow\n",
    "\n",
    "## 1. Initialization and State Management\n",
    "\n",
    "The workflow begins when `app.invoke()` is called with an initial state containing the user’s question and an empty message history.\n",
    "\n",
    "- **AgentState** is defined as a TypedDict that maintains:\n",
    "  - The user question\n",
    "  - The conversation history\n",
    "  - Intermediate tool outputs (`internal_rag_result`, `external_wiki_result`, `external_arxiv_result`)\n",
    "  - The final synthesized answer (`final_answer_text`)\n",
    "\n",
    "## 2. Entry Point: Internal Knowledge Base Node\n",
    "\n",
    "The graph starts at the internal knowledge base node.\n",
    "\n",
    "- The `rag_node` function is executed.\n",
    "- It calls `run_internal_rag` with the user question.\n",
    "- The retriever queries the FAISS vector store, which contains data from PDFs, web documentation, and previously loaded sources.\n",
    "- Retrieved chunks are combined with the question and passed to a RAG prompt.\n",
    "- The language model generates an initial answer.\n",
    "- The state is updated with a status message and the result is stored as `internal_rag_result`.\n",
    "\n",
    "## 3. Routing Decision\n",
    "\n",
    "After the internal RAG step, the workflow reaches a conditional routing function.\n",
    "\n",
    "- The `should_continue` function evaluates the sufficiency of `internal_rag_result`.\n",
    "- A routing prompt is sent to the language model, which must return one of the following decisions:\n",
    "  - `synthesize`\n",
    "  - `wikipedia_search`\n",
    "  - `arxiv_search`\n",
    "\n",
    "### Decision Logic\n",
    "\n",
    "- If the internal result sufficiently answers the question, the router selects `synthesize`.\n",
    "- If additional general knowledge is required, it selects `wikipedia_search`.\n",
    "- If academic or research-oriented information is needed, it selects `arxiv_search`.\n",
    "\n",
    "The routing decision determines the next node in the graph.\n",
    "\n",
    "## 4. External Tool Invocation\n",
    "\n",
    "Depending on the routing decision, the workflow may invoke an external tool.\n",
    "\n",
    "### Wikipedia Path\n",
    "\n",
    "- The workflow transitions to the Wikipedia node.\n",
    "- The `wikipedia_search` tool is invoked.\n",
    "- Wikipedia content is loaded, formatted, and stored in `external_wiki_result`.\n",
    "- The state is updated with a completion message.\n",
    "\n",
    "### Arxiv Path\n",
    "\n",
    "- The workflow transitions to the Arxiv node.\n",
    "- The `arxiv_search` tool is invoked.\n",
    "- Academic papers are retrieved and formatted.\n",
    "- The result is stored in `external_arxiv_result`.\n",
    "- The state is updated with a completion message.\n",
    "\n",
    "## 5. Final Answer Synthesis\n",
    "\n",
    "Regardless of whether external tools were used, the workflow proceeds to the final answer node.\n",
    "\n",
    "- The `final_answer_node` combines:\n",
    "  - The original question\n",
    "  - The internal RAG result\n",
    "  - Any external Wikipedia or Arxiv results\n",
    "- A final synthesis prompt instructs the language model to:\n",
    "  - Produce a comprehensive and coherent answer\n",
    "  - Clearly explain the reasoning\n",
    "  - Include inline source citations such as `[PDF]`, `[Web]`, `[Wikipedia]`, and `[Arxiv]`\n",
    "- The final response is stored in `final_answer_text`.\n",
    "\n",
    "## 6. End of Workflow\n",
    "\n",
    "After the final answer is generated, the graph transitions to the end state, completing the execution.\n",
    "\n",
    "# Output Clarity and Structure\n",
    "\n",
    "Throughout execution, structured status messages provide clear visibility into which node is running and what decisions are being made. This improves transparency, traceability, and debuggability.\n",
    "\n",
    "The final output is always a well-structured, comprehensive answer that clearly cites all sources used and explains how the information was selected and combined. This modular, state-driven architecture enables a flexible and adaptive RAG system that dynamically adjusts its strategy based on the query and available information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkxRWoHFJJ6n",
    "outputId": "21715364-0799-4b39-ca89-37994616c0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/490.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "pip install -q --upgrade langchain langchain-core langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVCzu9UbSO9e",
    "outputId": "09854938-516e-4509-de9b-b8ee19506888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q uvicorn langserve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tqf199viJQwn",
    "outputId": "f3115a5a-b4c6-4690-edfa-b9cb2216bb40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.6\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xybhwpPRRbqD"
   },
   "outputs": [],
   "source": [
    "pip install -q fastapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2nWgeMzeNKU",
    "outputId": "187de592-ff0a-42dc-948a-215b449b3075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/137.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip install --q langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-errJN57lVQ",
    "outputId": "f2df2c9b-fb9a-4ca5-a8ae-aff130a20b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip install -q pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-wv1n7w8rwH",
    "outputId": "7e2b1f4e-defb-4069-d454-b48f6cbe681f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip install -q streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWKLI5FM7zVl"
   },
   "source": [
    "## RAG dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uC-8r7Mb72eK",
    "outputId": "ded4b74c-0381-470a-9c13-b1aaab82e709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "pip install -q pypdf arxiv wikipedia faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ov5KTObUZwa3",
    "outputId": "643cc458-b811-460c-9636-9a97cf5c8eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LANGCHAIN_PROJECT secret not found in Colab userdata.\n",
      "Please add 'LANGCHAIN_PROJECT' to your Colab secrets if you intend to use Langsmith project tracking.\n",
      "\n",
      "--- Sanity Checks ---\n",
      "[OK] LANGCHAIN_API_KEY is set\n",
      "[OK] LANGCHAIN_TRACING_V2 is set\n",
      "[MISSING] LANGCHAIN_PROJECT is NOT set\n"
     ]
    }
   ],
   "source": [
    "# Google Colab-compatible environment setup with sanity checks\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "from google.colab.userdata import SecretNotFoundError # Import SecretNotFoundError\n",
    "\n",
    "# Fetch secrets from Colab userdata\n",
    "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\")\n",
    "try:\n",
    "    LANGCHAIN_PROJECT = userdata.get(\"LANGCHAIN_PROJECT\")\n",
    "except SecretNotFoundError:\n",
    "    print(\"Warning: LANGCHAIN_PROJECT secret not found in Colab userdata.\")\n",
    "    print(\"Please add 'LANGCHAIN_PROJECT' to your Colab secrets if you intend to use Langsmith project tracking.\")\n",
    "    LANGCHAIN_PROJECT = None # Set to None if not found\n",
    "\n",
    "# Set environment variables\n",
    "if LANGCHAIN_API_KEY:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "if LANGCHAIN_PROJECT:\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT\n",
    "\n",
    "# -------- Sanity Checks --------\n",
    "def sanity_check():\n",
    "    checks = {\n",
    "        \"LANGCHAIN_API_KEY\": os.environ.get(\"LANGCHAIN_API_KEY\"),\n",
    "        \"LANGCHAIN_TRACING_V2\": os.environ.get(\"LANGCHAIN_TRACING_V2\"),\n",
    "        \"LANGCHAIN_PROJECT\": os.environ.get(\"LANGCHAIN_PROJECT\"), # Check if it's set in env\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- Sanity Checks ---\")\n",
    "    for key, value in checks.items():\n",
    "        if value:\n",
    "            print(f\"[OK] {key} is set\")\n",
    "        else:\n",
    "            print(f\"[MISSING] {key} is NOT set\")\n",
    "\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X88GM2vJLPy6"
   },
   "source": [
    "# **All models available in GROQ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrwgdaiZKwoH",
    "outputId": "737e37bf-828a-4ffd-966b-5496ad42371b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"canopylabs/orpheus-arabic-saudi\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1765926439,\n",
      "      \"owned_by\": \"Canopy Labs\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 4000,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 50000\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"llama-3.3-70b-versatile\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1733447754,\n",
      "      \"owned_by\": \"Meta\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 32768\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"meta-llama/llama-prompt-guard-2-22m\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748632101,\n",
      "      \"owned_by\": \"Meta\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 512,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 512\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"meta-llama/llama-guard-4-12b\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1746743847,\n",
      "      \"owned_by\": \"Meta\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 1024\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"groq/compound-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756949707,\n",
      "      \"owned_by\": \"Groq\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 8192\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"canopylabs/orpheus-v1-english\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1766186316,\n",
      "      \"owned_by\": \"Canopy Labs\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 4000,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 50000\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1743874824,\n",
      "      \"owned_by\": \"Meta\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 8192\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"allam-2-7b\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1737672203,\n",
      "      \"owned_by\": \"SDAIA\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 4096,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 4096\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"openai/gpt-oss-120b\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754408224,\n",
      "      \"owned_by\": \"OpenAI\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 65536\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"meta-llama/llama-prompt-guard-2-86m\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748632165,\n",
      "      \"owned_by\": \"Meta\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 512,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 512\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"moonshotai/kimi-k2-instruct\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1752435491,\n",
      "      \"owned_by\": \"Moonshot AI\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 16384\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"whisper-large-v3-turbo\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1728413088,\n",
      "      \"owned_by\": \"OpenAI\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 448,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 448\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1743877158,\n",
      "      \"owned_by\": \"Meta\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 8192\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"qwen/qwen3-32b\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748396646,\n",
      "      \"owned_by\": \"Alibaba Cloud\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 40960\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"moonshotai/kimi-k2-instruct-0905\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1757046093,\n",
      "      \"owned_by\": \"Moonshot AI\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 262144,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 16384\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"whisper-large-v3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1693721698,\n",
      "      \"owned_by\": \"OpenAI\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 448,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 448\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"llama-3.1-8b-instant\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1693721698,\n",
      "      \"owned_by\": \"Meta\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 131072\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"openai/gpt-oss-20b\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754407957,\n",
      "      \"owned_by\": \"OpenAI\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 65536\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"groq/compound\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756949530,\n",
      "      \"owned_by\": \"Groq\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 8192\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"openai/gpt-oss-safeguard-20b\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1761708789,\n",
      "      \"owned_by\": \"OpenAI\",\n",
      "      \"active\": true,\n",
      "      \"context_window\": 131072,\n",
      "      \"public_apps\": null,\n",
      "      \"max_completion_tokens\": 65536\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from google.colab import userdata\n",
    "\n",
    "# Ensure GROQ_API_KEY is fetched directly from Colab secrets or environment\n",
    "api_key = userdata.get(\"GROQ_API_KEY\")\n",
    "\n",
    "# If the API key is still not found, raise an error or inform the user\n",
    "if not api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not found in Colab secrets. Please ensure it is added.\")\n",
    "\n",
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status() # This will raise an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6tU0Xu5L9fF"
   },
   "source": [
    "# Model Selection Guide (Purpose-Based)\n",
    "\n",
    "This guide maps each available model to its best use case so you can quickly choose the right one.\n",
    "\n",
    "---\n",
    "\n",
    "## General Natural Language Generation / Chat\n",
    "\n",
    "Suitable for chatbots, summaries, reasoning, coding help, and general text generation.\n",
    "\n",
    "| Model | Notes | Best For |\n",
    "|-----|-----|-----|\n",
    "| **llama-3.3-70b-versatile** | Large, high-quality | Deep reasoning, complex tasks, long contexts |\n",
    "| **llama-3.1-8b-instant** | Small, very fast | General chat, Q&A, lightweight apps |\n",
    "| **openai/gpt-oss-20b** | Open-source GPT-style | Strong general text generation |\n",
    "| **openai/gpt-oss-120b** | Very large OSS model | Highest-quality OSS reasoning & generation |\n",
    "\n",
    "---\n",
    "\n",
    "## Lightweight / Fast / Cost-Efficient\n",
    "\n",
    "Optimized for speed and lower resource usage.\n",
    "\n",
    "| Model | Notes | Best For |\n",
    "|-----|-----|-----|\n",
    "| **groq/compound-mini** | Lightweight | Fast throughput, low cost |\n",
    "| **groq/compound** | Balanced | Speed + quality |\n",
    "| **allam-2-7b** | 7B model | Very lightweight text generation |\n",
    "| **moonshotai/kimi-k2-instruct** | Instruction-tuned | Fast assistant-style tasks |\n",
    "\n",
    "---\n",
    "\n",
    "## Long-Context Processing\n",
    "\n",
    "Designed for very large documents and multi-file inputs.\n",
    "\n",
    "| Model | Context Size | Best For |\n",
    "|-----|-----|-----|\n",
    "| **moonshotai/kimi-k2-instruct-0905** | 262k tokens | Books, long documents, multi-doc reasoning |\n",
    "| **llama-3.1 / 3.3 variants** | 131k tokens | Long-context chat and analysis |\n",
    "\n",
    "---\n",
    "\n",
    "## Speech-to-Text (Not Text Generation)\n",
    "\n",
    "| Model | Best For |\n",
    "|-----|-----|\n",
    "| **whisper-large-v3** | High-quality transcription |\n",
    "| **whisper-large-v3-turbo** | Faster speech-to-text |\n",
    "\n",
    "---\n",
    "\n",
    "## Safety / Guard Models (Not for Generation)\n",
    "\n",
    "Used only for moderation, safety checks, or filtering.\n",
    "\n",
    "| Model | Purpose |\n",
    "|-----|-----|\n",
    "| **meta-llama/llama-guard-4-12b** | Safety classification |\n",
    "| **meta-llama/llama-prompt-guard-2-22m / 86m** | Prompt risk detection |\n",
    "\n",
    "---\n",
    "\n",
    "## Language / Region-Specific\n",
    "\n",
    "| Model | Best For |\n",
    "|-----|-----|\n",
    "| **canopylabs/orpheus-v1-english** | English-focused NLP |\n",
    "| **canopylabs/orpheus-arabic-saudi** | Arabic (Saudi dialect) |\n",
    "| **allam-2-7b** | Arabic-centric lightweight tasks |\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Recommendations\n",
    "\n",
    "- **Best overall (small + free):** `llama-3.1-8b-instant`\n",
    "- **Best quality:** `llama-3.3-70b-versatile`\n",
    "- **Fastest / cheapest:** `groq/compound-mini`\n",
    "- **Very long documents:** `moonshotai/kimi-k2-instruct-0905`\n",
    "- **Speech recognition:** `whisper-large-v3`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHRQYOf4Zwa3",
    "outputId": "9e43dd82-48f2-45df-daa4-76eaf7fb9b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True} client=<groq.resources.chat.completions.Completions object at 0x7eb88bf985f0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7eb88b6724e0> model_name='llama-3.1-8b-instant' temperature=1e-08 model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# Set Groq API key (must exist in Colab secrets)\n",
    "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
    "\n",
    "# Initialize Groq LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8GO_xe-BUUG"
   },
   "source": [
    "## **Sanity check: verify the Groq LLM is working**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhBEwUj7Zwa3",
    "outputId": "82c7ece8-5312-4bc0-a922-1d52075ad0e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: OK\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = llm.invoke([HumanMessage(content=\"Reply with the single word: OK\")])\n",
    "\n",
    "print(\"LLM response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VHnW4-pJZwa3"
   },
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1OeZ-AhZwa4",
    "outputId": "1e230777-6de7-4a29-b97a-32909c231cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Generative AI refers to a subset of artificial intelligence (AI) that focuses on generating new, original content, such as text, images, music, or videos, based on patterns and structures learned from existing data. This type of AI uses algorithms and machine learning techniques to create new, unique outputs that are often indistinguishable from those created by humans.\\n\\nGenerative AI models are trained on large datasets, which allows them to learn the underlying patterns, styles, and structures of the data. Once trained, these models can generate new content that is similar in style and quality to the original data.\\n\\nSome common applications of generative AI include:\\n\\n1. **Text generation**: Generating text, such as articles, stories, or chatbot responses, that are coherent and engaging.\\n2. **Image generation**: Creating new images, such as artwork, landscapes, or product designs, that are realistic and visually appealing.\\n3. **Music generation**: Composing new music, such as melodies, harmonies, or entire songs, that are original and engaging.\\n4. **Video generation**: Creating new videos, such as animations, simulations, or movie clips, that are realistic and engaging.\\n5. **Data augmentation**: Generating new data points to augment existing datasets, which can improve the performance of machine learning models.\\n\\nSome popular examples of generative AI include:\\n\\n1. **DeepDream**: A neural network that generates surreal and dreamlike images by applying a set of filters to an input image.\\n2. **GANs (Generative Adversarial Networks)**: A type of neural network that generates new images, videos, or text by competing with a discriminator network to produce realistic outputs.\\n3. **Language models**: Such as BERT, RoBERTa, and XLNet, which generate text based on patterns and structures learned from large datasets.\\n4. **Style transfer**: A technique that generates new images or videos by applying the style of one image to another.\\n\\nGenerative AI has many potential applications, including:\\n\\n1. **Content creation**: Generating new content, such as articles, videos, or music, that can be used in marketing, entertainment, or education.\\n2. **Data augmentation**: Generating new data points to augment existing datasets, which can improve the performance of machine learning models.\\n3. **Art and design**: Generating new artwork, designs, or products that are original and visually appealing.\\n4. **Personalization**: Generating personalized content, such as product recommendations or chatbot responses, that are tailored to individual users.\\n\\nHowever, generative AI also raises several concerns, including:\\n\\n1. **Authenticity**: Determining whether generated content is authentic or not.\\n2. **Bias**: Ensuring that generated content does not perpetuate biases or stereotypes.\\n3. **Intellectual property**: Addressing issues related to ownership and copyright of generated content.\\n4. **Job displacement**: The potential impact of generative AI on jobs, such as content creation, writing, or design.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 599, 'prompt_tokens': 41, 'total_tokens': 640, 'completion_time': 1.07667437, 'completion_tokens_details': None, 'prompt_time': 0.003729961, 'prompt_tokens_details': None, 'queue_time': 0.084695116, 'total_time': 1.080404331}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6c980774ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bdeb5-51c3-7441-a458-c07ecbe05eb1-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 41, 'output_tokens': 599, 'total_tokens': 640})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkWDIlcyQ1m_"
   },
   "source": [
    "## **Sanity Check 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EqkGPmt6gLVt",
    "outputId": "8b3b246b-77db-4f2b-e936-e8307fe900ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Prithu. As a learner AI Engineer, you're likely exploring the exciting world of artificial intelligence and machine learning. What specific areas of AI are you interested in or currently learning about? Are you working on any projects or looking for resources to help you improve your skills? I'm here to help and provide any guidance I can.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 51, 'total_tokens': 124, 'completion_time': 0.127391292, 'completion_tokens_details': None, 'prompt_time': 0.004806736, 'prompt_tokens_details': None, 'queue_time': 0.078197526, 'total_time': 0.132198028}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6b5c123dd9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bdeb5-575b-7811-9e7f-73bb93ba4efb-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 51, 'output_tokens': 73, 'total_tokens': 124})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "llm.invoke([HumanMessage(content=\"Hi , My name is Prithu and I am a Learner AI Engineer\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myMiU8quLbEo"
   },
   "source": [
    "# **Langchain_RAG_AGENT_ConversationalQA_with_Memory_History**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af9c1d9c"
   },
   "source": [
    "# Langchain_RAG capabilities, including loaders for PDF, ArXiv, and Wikipedia, the FAISS vector store, and Sentence Transformers for embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ea37e54"
   },
   "source": [
    "## Load and Process Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "m_D1fp9uicPe"
   },
   "outputs": [],
   "source": [
    "## Arxiv--Research\n",
    "## Tools creation\n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper,ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EvIWhmCEikSP",
    "outputId": "0ae6fe9c-92de-4d7c-ca11-b8a702256fff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Used the inbuilt tool of wikipedia\n",
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmB4rO8HikFM",
    "outputId": "28d7ae66-ad2e-4d1b-ca4b-1253ad3a5674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "print(arxiv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0hdPdcTPiwF1"
   },
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCYOZ7WZjzYJ"
   },
   "source": [
    "## Custom tools[RAG Tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iwPpTtmizY1",
    "outputId": "b405020c-5abe-4896-830a-094c0d572b41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Custom tools[RAG Tool]\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b021f529",
    "outputId": "c4ed3e34-fd04-4044-f5aa-2077ec9876e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/210.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/210.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip install -q --upgrade langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vPrXEbyStBqz"
   },
   "outputs": [],
   "source": [
    "# Install the missing library\n",
    "!pip install -q langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386,
     "referenced_widgets": [
      "56a517b056a04247bf1abd9c6c65ac90",
      "1f0182f39fc14fe58ee87d85f6f8c964",
      "bfa11305e00740ce8a915e4fb8812f67",
      "617fc18476b142acadaaf57f73ba25a6",
      "d0ebf05c30d54538ac9d0ac246a35c45",
      "a967fe988b284d55a5f74d54ef41cf33",
      "45aec8252a5a4f248633cdea6d5194a0",
      "3945dd3eae3e4163a08ab728a8d1fc9b",
      "e3f7b8000c7d4fa6bf5431ab94366c46",
      "74afb180b663459587ae7f3a64778433",
      "5b397c78492b4a39aa5d9508d93fa487",
      "8b0f0d7143174faabfbda62931355b3b",
      "808673ea49094943bf3048ae08aaf192",
      "4d6e5ada70ed4686be1cd3d360fd91f2",
      "19002c6461e647dfb0815fcb5bcdc82c",
      "a5852a337a0b49609f1d078d493b2ca5",
      "23228643351b401ca42bd537fe498da6",
      "95eea49a0eac48c7b487af329dc44a75",
      "bb64808e8cb8487ebb5fb7141cc55a1b",
      "1d98ae93beac42db859922e6d8ab2af9",
      "a5bdc7b31803403eb1d3d4542febdbf5",
      "0c0e2a5a81c54dcdbcceabea265e2d71",
      "ce520fd74a7848f9ae8c5b86a35e96a9",
      "fcf5d9ec89854bfda49aff9fcd1d458f",
      "9fb0298679174e80b71db152170d3edf",
      "43dce10740294f3c9349dd069e4268ce",
      "1de05e1c9e82446a8bff1517a4abdbd7",
      "1d4c3189f5314056b3e182458b8ddd50",
      "e6b429e198984b2b9301b4b4099a65d0",
      "e5249f5176cb466d9a77e8df33decdc1",
      "1a29d923765c41279220a726876236e4",
      "0849166113034da6b0ea088e6fef39a2",
      "6e931a2c3ceb4251a4923dbe94ac14b3",
      "b41f772389b149c48a4d6dc624f9b6e0",
      "b8693bdd6c734c5285bb174930d131b6",
      "9ae1a176f390481aab265d1329cc2ee8",
      "a91c668b7bc946d1b0c995eed1c6810c",
      "2bc346628aa848ee8151384d22bea6d8",
      "c61881c2cc8d48baa9e1c8d8bd006e80",
      "6d49e7316ec046a1a255fc4138149a00",
      "35e97715c53e43d8a0f7136793038644",
      "9b3dfd08c3984592b8dd7fff2469171f",
      "007f51e6bffe4b369e71daeb0a8c9e26",
      "21db3999a10e4d93afd76f42899d92f1",
      "88ed2bcb8d474198b0ded0979ebac511",
      "d25001e9da6a46eda83443a2bc0684be",
      "7d17df15ebff48419a7f6d9cd6d3422f",
      "c73f35bd427c4d49adb9d93fc2fed77a",
      "b6582eb1eb3348afb43fa04d8caa0f5e",
      "dccdbef154cc4f52a6d8f7fb017d75e7",
      "c5eca353bc1641ad93c80da398f4d957",
      "cedf97e455ea416b96cc36b1a8fd55a8",
      "bdb51a92512a4fbeb0234e668aa135af",
      "30c29d8b6c434c938085511ef5145768",
      "e43cfee656374af5ac63be294cb3e7c8",
      "b73b8e6862bc4fc3aa378313381fc698",
      "d3bc0516669f4269848888508a90357c",
      "81351cdb0a1442aab542f29e9571d36a",
      "3fc699e3daf64afcb3ba6d96856bfdac",
      "aa0fb53fa320495985f5142a952a77af",
      "6ebab31628e349eca6b9bce685760023",
      "2101ffa65a87433c9f7607ec71a12ffb",
      "ce81f06f76784616a4877f6a50f961fc",
      "0185d34823a14aceb2d9ec28f698f62b",
      "ea75c15f224e4efaa42d26f24fd2fed8",
      "9d4069c9788d42d1b0598acd9a71093f",
      "e175b453262a405a8a32907bf4c10a6f",
      "a990127729f148849406491d80c59bfc",
      "ce560826b80544fba8207752e098b379",
      "9bd4de4beec74c50b275e6109c814cf5",
      "06dc3d127aa741089b294d99f43749a3",
      "8148c819d2c146129ddb4a53d2609ebe",
      "a49955c3ada24c93aab2559906731cd6",
      "a13524389bc34a88b163ef5bff599fcd",
      "d1616c9940bd4a0fa6b678826d8d5572",
      "58ea4df2070b4ebbba76ef0e12fecbe6",
      "983cd248012d4701be2b6f4bf8d1178e",
      "78e88917df7d41d8929bdd79bd0be419",
      "5ef99d3aaa724b7bab8df82f8281d738",
      "d62ed719b61447db82025155fe94d0a8",
      "e6c39fb7dfa1422f8ad46e61b171569e",
      "767d00cd71fc430a98211977dbf29529",
      "5ffe56f398de4cefa20654e4522380bf",
      "edbca2f0a9f241419f5b76a36ae538fe",
      "53e2e84b21b1402689942cdc31b8d104",
      "7357d1344dd84a5aa41284107e1a2e9a",
      "23f9afa2f21c4c7a96917ff297a379e0",
      "9d92944159f34b60a4b9294171d16423",
      "9944ad247e104aca961bc8032da5068b",
      "1ef824884ded4cea84edeb67d206db08",
      "f777185ba08049db91291fcb2c36f67e",
      "ec2e3377902c41fd9bccbf30e4f4b4f4",
      "a5b7845390fc422d81d4b7465cf2bfeb",
      "3ffd45d6aa6b4d578d5da8ed24293149",
      "4a594e6127ab4e00ac1c78928ba65a46",
      "04494929c4344eb8b7644148b6cd131c",
      "9de66d7a04aa4eddab31d05dfdff01a8",
      "422e78a009184d31b73f15cdfeb0f327",
      "7ce1eeb81370401d86802ee26a2d2a9f",
      "5272682e2f314ae68a10e9a5981bda3e",
      "0d3c2e6de64d489dbf94376d17e601c9",
      "6b2003c373114053999aa9ae4ca530de",
      "8063e9488e5c4b239a782813e04b122b",
      "f11eb93b032741ee8cc2ad3e07047cd4",
      "92674bffe6624c4f80a544d5fd0720f9",
      "793c3c63728c4729ae684ba639c9de1e",
      "c75c0e4e56fa4502bb4daf488e3ae4a3",
      "4a283d6022f24dca8ded5659ed6602e8",
      "c88a408fd1ff4bdeab3da6342af3fc9b",
      "fba0c15ce8ad46a487c8aab4b8762035",
      "5bacec014089493bae9c9e32addd898d",
      "5105e01b2bbf4468a26e1eeb5eab3e94",
      "0f616c723b044005aebe9c6b721c890e",
      "6d5def57ba19448f8793775261f1d6a1",
      "6286f403e9954954b84c2740ce282d23",
      "6479f705f59848fbaff904db7543385b",
      "c4bd94f1a7464fd59a95fdff77e68f9c",
      "e2dda5bbfd7049cba3ed4b466632c4f7",
      "bb72da479c04439a9efb9010178094d7",
      "2bd15d8aba3245cea2af5c7fe4d46c85",
      "36000cb23e714b2987417c64a6804da8"
     ]
    },
    "id": "o-v8gLoki-_8",
    "outputId": "a1d70dec-6768-47f9-d8e1-0ede4b244736"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a517b056a04247bf1abd9c6c65ac90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0f0d7143174faabfbda62931355b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce520fd74a7848f9ae8c5b86a35e96a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41f772389b149c48a4d6dc624f9b6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ed2bcb8d474198b0ded0979ebac511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73b8e6862bc4fc3aa378313381fc698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e175b453262a405a8a32907bf4c10a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e88917df7d41d8929bdd79bd0be419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9944ad247e104aca961bc8032da5068b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5272682e2f314ae68a10e9a5981bda3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bacec014089493bae9c9e32addd898d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7eb7f04ccfe0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# 1. Load web docs\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Chunk documents\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=50\n",
    ").split_documents(docs)\n",
    "\n",
    "# 3. Hugging Face embeddings (recommended lightweight model)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "# 4. Vector store\n",
    "vectordb = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# 5. Retriever\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TL7jrxnbrbpL"
   },
   "outputs": [],
   "source": [
    "# Step 0: Fix environment for Arxiv loader\n",
    "# !pip install \"PyMuPDF<1.22\" langchain langchain_community langchain_groq faiss-cpu transformers\n",
    "\n",
    "from typing import List, TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from IPython.display import Image # Added for graph visualization\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WikipediaLoader, ArxivLoader, WebBaseLoader\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KwQKeJXwJsI"
   },
   "source": [
    "## **LLM Setup and Load Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xvF9JXVcjKmZ"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. LLM Setup\n",
    "# -------------------------------\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=1000  # Increased max_tokens for a more comprehensive summary\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load Sources\n",
    "# -------------------------------\n",
    "def load_pdf(file_path: str) -> List[Document]:\n",
    "    try:\n",
    "        # Assuming /content/LLM.pdf is text, not binary PDF. Add error handling for encoding.\n",
    "        with open(file_path, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        return [Document(page_content=content, metadata={\"source\": file_path, \"source_tag\": \"[PDF]\"})]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_web_docs(url: str) -> List[Document]:\n",
    "    try:\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source_tag\"] = \"[Web]\"\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading web docs from {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_wikipedia(query: str, max_docs: int = 2) -> List[Document]:\n",
    "    try:\n",
    "        loader = WikipediaLoader(query=query, load_max_docs=max_docs)\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source_tag\"] = \"[Wikipedia]\"\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Wikipedia load error: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_arxiv(query: str, max_docs: int = 2) -> List[Document]:\n",
    "    try:\n",
    "        loader = ArxivLoader(query=query, load_max_docs=max_docs)\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source_tag\"] = \"[Arxiv]\"\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Arxiv load error: {e}\")\n",
    "        return []\n",
    "\n",
    "pdf_docs = load_pdf(\"/content/LLM.pdf\")\n",
    "web_docs = load_web_docs(\"https://docs.smith.langchain.com/\")\n",
    "wiki_docs = load_wikipedia(\"LangChain\")\n",
    "arxiv_docs = load_arxiv(\"LangChain\")\n",
    "all_docs = pdf_docs + web_docs + wiki_docs + arxiv_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeTKYrUGzNEQ"
   },
   "source": [
    "## **Chunk Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "cb379d1de03b44a7889de43cb8eddb6b",
      "5cb4b1c7a086412dab08a7948ec0df7b",
      "16f6848b2bf84b4e998a1b26af0c7b84",
      "f9259b280bc24b08b1dcbf6bda965a2d",
      "686156d6e04b40e08088b484add50450",
      "1cd5aa69e0a145d9b65f3683e6d506c1",
      "56c6230e02de4899b64ac0c091b96eb7",
      "a7b83c4dc5e14e47b819ec2faaa0953b",
      "011ad7cad40540288325e86997fdd193",
      "0dbe41faafd741619b65b7c82a20a071",
      "d17af915fbe2493d92823695d305d770"
     ]
    },
    "id": "DnKtVkJirwNF",
    "outputId": "f99a4351-9c04-4dee-d4ac-09138435bc20"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb379d1de03b44a7889de43cb8eddb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3. Chunk Documents\n",
    "# -------------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Embeddings & Vector Store\n",
    "# -------------------------------\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    show_progress=True # Added to show progress during embedding generation\n",
    ")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bMJi5hAjr4mq"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Format Documents\n",
    "# -------------------------------\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    if not docs:\n",
    "        return \"No results found.\"\n",
    "    formatted_content = []\n",
    "    for d in docs:\n",
    "        source_tag = d.metadata.get('source_tag','[Unknown]')\n",
    "        # Use a consistent structure for each document and its source\n",
    "        formatted_content.append(f\"Content: {d.page_content[:500]} (Source: {source_tag})\")\n",
    "    return \"\\n\\n\".join(formatted_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LhBTtj6zIfY"
   },
   "source": [
    "## **Tool and  RAG Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_CSqDr9erk3B"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6. Tools\n",
    "# -------------------------------\n",
    "@tool(description=\"Searches the internal knowledge base (vector store) for relevant documents. Returns content with inline citations from PDF, Web, Wikipedia, Arxiv.\")\n",
    "def internal_knowledge_base(query: str) -> str:\n",
    "    print(f\"\\n Calling Internal Knowledge Base for: {query}\")\n",
    "    docs = retriever.invoke(query)\n",
    "    result = format_docs(docs)\n",
    "    print(f\" Internal KB result: {result[:100]}...\")\n",
    "    return result\n",
    "\n",
    "@tool(description=\"Searches Wikipedia for the given query and returns summarized text with source tags.\")\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    print(f\"\\n Calling Wikipedia Search for: {query}\")\n",
    "    docs = load_wikipedia(query)\n",
    "    result = format_docs(docs)\n",
    "    print(f\" Wikipedia result: {result[:100]}...\")\n",
    "    return result\n",
    "\n",
    "@tool(description=\"Searches Arxiv for papers matching the query and returns summarized text with source tags.\")\n",
    "def arxiv_search(query: str) -> str:\n",
    "    print(f\"\\n Calling Arxiv Search for: {query}\")\n",
    "    docs = load_arxiv(query)\n",
    "    result = format_docs(docs)\n",
    "    print(f\" Arxiv result: {result[:100]}...\")\n",
    "    return result\n",
    "\n",
    "# -------------------------------\n",
    "# 7. RAG Prompt\n",
    "# -------------------------------\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question based ONLY on the following context. Cite sources inline using their tags (e.g., [PDF], [Web], [Wikipedia], [Arxiv]). If the context is insufficient, state that you cannot answer from the provided information.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT-kSdsSzEB1"
   },
   "source": [
    "## **RAG Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "SFOyRaCkrk1x"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8. RAG Chain\n",
    "# -------------------------------\n",
    "def run_internal_rag(question: str) -> str:\n",
    "    # Directly invoke the retriever and format docs within this function\n",
    "    # to ensure the context passed to the LLM is correctly formatted.\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    context = format_docs(retrieved_docs)\n",
    "\n",
    "    # Check if context explicitly indicates insufficiency\n",
    "    if \"No results found.\" in context:\n",
    "        return \"Based on the internal knowledge base, I don't have enough information to answer this question.\"\n",
    "\n",
    "    result = rag_prompt | llm | StrOutputParser()\n",
    "    return result.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Agent State\n",
    "# -------------------------------\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    question: str\n",
    "    internal_rag_result: str\n",
    "    external_wiki_result: str\n",
    "    external_arxiv_result: str\n",
    "    final_answer_text: str\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Nodes\n",
    "# -------------------------------\n",
    "def rag_node(state: AgentState):\n",
    "    print(\"\\n Entering Internal KB Node\")\n",
    "    answer = run_internal_rag(state[\"question\"])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Internal RAG search completed.\")],\n",
    "        \"internal_rag_result\": answer\n",
    "    }\n",
    "\n",
    "def wiki_node(state: AgentState):\n",
    "    print(\"\\n Entering Wikipedia Node\")\n",
    "    # Invoke the tool directly, as it handles its own context loading/formatting\n",
    "    answer = wikipedia_search.invoke({\"query\": state[\"question\"]})\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Wikipedia search completed.\")],\n",
    "        \"external_wiki_result\": answer\n",
    "    }\n",
    "\n",
    "def arxiv_node(state: AgentState):\n",
    "    print(\"\\n Entering Arxiv Node\")\n",
    "    # Invoke the tool directly, as it handles its own context loading/formatting\n",
    "    answer = arxiv_search.invoke({\"query\": state[\"question\"]})\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Arxiv search completed.\")],\n",
    "        \"external_arxiv_result\": answer\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uop4qxF1yXUn"
   },
   "source": [
    "## **Final Answer Synthesis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbecd3e9",
    "outputId": "8bedf207-05d1-486f-c761-291c97c75431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangGraph agent compiled successfully!\n",
      "\n",
      "--- LangGraph Visual ---\n",
      "Error displaying graph visual: Install pygraphviz to draw graphs: `pip install pygraphviz`.\n",
      "Please ensure 'graphviz' is installed (e.g., !apt-get install -y graphviz) and its dependencies are met.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 11. Final Answer Synthesis\n",
    "# -------------------------------\n",
    "final_answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an intelligent assistant. Synthesize the following information to provide a comprehensive answer to the user's question.\n",
    "               Clearly cite sources like (Source: [PDF]), (Source: [Web]), (Source: [Wikipedia]), or (Source: [Arxiv]) when information is used.\n",
    "               If a source isn't explicitly mentioned, state it as 'General knowledge' or 'Synthesized from multiple sources'.\n",
    "               If no relevant information is found across all sources, state that you cannot answer the question.\n",
    "               Explain your reasoning for using specific information.\n",
    "\n",
    "               Original Question: {question}\n",
    "               Internal RAG Result: {internal_rag_result}\n",
    "               Wikipedia Result: {external_wiki_result}\n",
    "               Arxiv Result: {external_arxiv_result}\n",
    "               Math Result: {math_result}\"\"\"),\n",
    "    (\"human\", \"Generate final comprehensive answer with proper citations and explain your reasoning.\")\n",
    "])\n",
    "\n",
    "def final_answer_node(state: AgentState):\n",
    "    print(\"\\n Entering Final Answer Node\")\n",
    "    context = {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"internal_rag_result\": state.get(\"internal_rag_result\", \"No results from internal RAG.\"),\n",
    "        \"external_wiki_result\": state.get(\"external_wiki_result\", \"No results from Wikipedia.\"),\n",
    "        \"external_arxiv_result\": state.get(\"external_arxiv_result\", \"No results from Arxiv.\"),\n",
    "        \"math_result\": state.get(\"math_result\", \"No math calculation performed.\")\n",
    "    }\n",
    "    final_answer = (final_answer_prompt | llm | StrOutputParser()).invoke(context)\n",
    "    print(f\" Final Answer generated: {final_answer[:100]}...\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=final_answer)],\n",
    "        \"final_answer_text\": final_answer\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Router LLM\n",
    "# -------------------------------\n",
    "llm_router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a routing assistant. Based on the user's question and the initial internal RAG result, decide the next step.\n",
    "\n",
    "               Respond with ONLY ONE of these exact words (lowercase):\n",
    "               - synthesize (if internal RAG has sufficient information and no further search is needed)\n",
    "               - wikipedia (if you need general encyclopedia information to supplement or clarify)\n",
    "               - arxiv (if you need academic research papers to supplement or clarify)\n",
    "               - math_calculator (if a mathematical expression needs to be evaluated)\n",
    "\n",
    "               Question: {question}\n",
    "               Internal RAG Result: {internal_rag_result}\"\"\"),\n",
    "    (\"human\", \"What should be the next step?\")\n",
    "])\n",
    "\n",
    "llm_router = llm_router_prompt | llm | StrOutputParser()\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"synthesize\",\"wikipedia_search\",\"arxiv_search\", \"math_calculator\"]:\n",
    "    question = state[\"question\"]\n",
    "    internal_rag_result = state.get(\"internal_rag_result\",\"\\nI cannot answer the question based on the provided context.\") # Default to 'cannot answer' if empty\n",
    "\n",
    "    print(f\"\\n Deciding next step with Router LLM for question: {question[:50]}...\")\n",
    "    print(f\"  Internal RAG snippet: {internal_rag_result[:100]}...\")\n",
    "\n",
    "    decision = llm_router.invoke({\n",
    "        \"question\": question,\n",
    "        \"internal_rag_result\": internal_rag_result\n",
    "    }).lower().strip()\n",
    "\n",
    "    print(f\"  Router decision: {decision}\")\n",
    "\n",
    "    if \"synthesize\" in decision:\n",
    "        return \"Final Answer\"\n",
    "    elif \"wikipedia\" in decision:\n",
    "        return \"Wikipedia\"\n",
    "    elif \"arxiv\" in decision:\n",
    "        return \"Arxiv\"\n",
    "    elif \"math_calculator\" in decision:\n",
    "        return \"Math_Calculator\"\n",
    "    else:\n",
    "        # Fallback if LLM gives unexpected output (e.g., if it hallucinates a tool name)\n",
    "        print(\"  Router produced unexpected decision, defaulting to Wikipedia if RAG insufficient, else synthesize.\")\n",
    "        if \"i don't have enough information\" in internal_rag_result.lower() or \"no results found\" in internal_rag_result.lower() or \"cannot answer the question\" in internal_rag_result.lower():\n",
    "            return \"Wikipedia\"\n",
    "        return \"Final Answer\"\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Agent State (UPDATED)\n",
    "# -------------------------------\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    question: str\n",
    "    internal_rag_result: str\n",
    "    external_wiki_result: str\n",
    "    external_arxiv_result: str\n",
    "    math_result: str # Added for math calculations\n",
    "    final_answer_text: str\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Nodes (UPDATED)\n",
    "# -------------------------------\n",
    "# existing nodes (rag_node, wiki_node, arxiv_node) remain the same\n",
    "\n",
    "def math_node(state: AgentState):\n",
    "    print(\"\\n Entering Math Node\")\n",
    "    # The math_calculator tool expects an expression string\n",
    "    answer = math_calculator.invoke({\"expression\": state[\"question\"]}) # Use question as expression\n",
    "    print(f\" Math result: {answer[:100]}...\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Math calculation completed.\")],\n",
    "        \"math_result\": answer\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 13. Build LangGraph (UPDATED)\n",
    "# -------------------------------\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"Internal KB\", rag_node)\n",
    "graph.add_node(\"Wikipedia\", wiki_node)\n",
    "graph.add_node(\"Arxiv\", arxiv_node)\n",
    "graph.add_node(\"Math_Calculator\", math_node) # Added math node\n",
    "graph.add_node(\"Final Answer\", final_answer_node)\n",
    "\n",
    "graph.set_entry_point(\"Internal KB\")\n",
    "\n",
    "graph.add_conditional_edges(\"Internal KB\", should_continue, {\n",
    "    \"Final Answer\": \"Final Answer\",\n",
    "    \"Wikipedia\": \"Wikipedia\",\n",
    "    \"Arxiv\": \"Arxiv\",\n",
    "    \"Math_Calculator\": \"Math_Calculator\" # Added conditional edge for math\n",
    "})\n",
    "\n",
    "graph.add_edge(\"Wikipedia\", \"Final Answer\")\n",
    "graph.add_edge(\"Arxiv\", \"Final Answer\")\n",
    "graph.add_edge(\"Math_Calculator\", \"Final Answer\") # Added edge from math to final answer\n",
    "graph.add_edge(\"Final Answer\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "print(\" LangGraph agent compiled successfully!\")\n",
    "\n",
    "# --- Display the graph visual ---\n",
    "print(\"\\n--- LangGraph Visual ---\")\n",
    "# Ensure graphviz is installed for drawing, e.g., !apt-get install -y graphviz\n",
    "# If you encounter issues, this step might require additional troubleshooting for your environment.\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying graph visual: {e}\")\n",
    "    print(\"Please ensure 'graphviz' is installed (e.g., !apt-get install -y graphviz) and its dependencies are met.\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 14. Test Queries (already existing tests below)\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d4c9d47aebe54b8186d2efff615c34fd",
      "2b42ac6e57c04ec4b933fb0a1baeb464",
      "ff54f180c5074ba4af25de17aab18b40",
      "2d336f7269a544f5b3b791359d6cd2d5",
      "b6d5274fe9424e04bb73398f1248cce6",
      "24beb7163afe4e00a185b44180492cdb",
      "6f586893ab084fda8b26b46ce0b05e2a",
      "2409069f3da04fc2a872350d6c0b7487",
      "35e96502951b4d16a647650fbc06645c",
      "1f70f13aaf9841a99bb3ddd7043bf778",
      "5cf74d0c02f54fc1aecc6ef13333362f",
      "cf8d107333eb46cdb2912031085bb87c",
      "adcc9206e2634741ba3ac7ab2ded2dd9",
      "9f3ea350e8f24e8988c26abdcbbc1272",
      "f0635abb95514788aa2f1e6c3dbf111c",
      "4d567a1b36a945278aebd367cb53a2e8",
      "7857bd6b61ad4d91bb728032cb997b9f",
      "5c7f3cc35ff640edb1f7572b02eb5507",
      "10e23d39f84f48a5b7896b1006a92604",
      "5720d9b506244bdc886765e48e323767",
      "3ac7ea2774b24fca8045d925c81f1e33",
      "00d29a5aa96e4acc92823f9701bf2ee2",
      "be29b585a56d40a2b154872d5a8328cd",
      "dd73043e410647d4a9f78e7026356f7e",
      "9d59cb5747a444abbdb55e9a7c58058e",
      "8fbc8db6f7be404db3f44239eb9526c8",
      "9ad4e2d9613c414092ed8fd19acec428",
      "df05d52be8dc4902956dc347c109fedc",
      "b7fa27e97249450ca0f04760f9bf40d0",
      "4e62abb237cb4887a6c127e58abb339c",
      "51810e1273114865b667eb3619cf1b69",
      "d40427a4b1194cc2b63d55d9ca595987",
      "76613bef30fe43febc53dccdb16d12ee",
      "559bce70c604437f93ef4c8abe925118",
      "fcae53f1970b4dafb331231780b59926",
      "b28f62e87b214a9e9760e70717a25d06",
      "332280adfdcb44ecba15e1c4aae62d64",
      "1286dfae583346b2bd29fcd848b11441",
      "cbcdab8971d2447ca933394c0fe3d9a6",
      "fabb17e607a749159fa573f4289dec58",
      "2f0b583d76dd4164afbbc47de2de2858",
      "f351c2b7e19746fbaac986ff7e210dfb",
      "2e31cec0d3004e02b72a56879ab10890",
      "a8c67b84459f46dcacdc832f58a2597f"
     ]
    },
    "id": "ZHf5PvDQrk0l",
    "outputId": "19817722-6657-4580-c9a2-a039ffa91276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangGraph agent compiled successfully!\n",
      "\n",
      "--- LangGraph Visual ---\n",
      "Error displaying graph visual: Install pygraphviz to draw graphs: `pip install pygraphviz`.\n",
      "Please ensure 'graphviz' is installed (e.g., !apt-get install -y graphviz) and its dependencies are met.\n",
      "\n",
      "--- Test Query 1: Internal RAG should be sufficient ---\n",
      "\n",
      " Entering Internal KB Node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c9d47aebe54b8186d2efff615c34fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Deciding next step with Router LLM for question: What are Large Language Models used for?...\n",
      "  Internal RAG snippet: According to the provided context, Large Language Models (LLMs) are used in various sectors, includi...\n",
      "  Router decision: synthesize\n",
      "\n",
      " Entering Final Answer Node\n",
      " Final Answer generated: Based on the provided information, Large Language Models (LLMs) are used in various sectors, includi...\n",
      "Final Synthesized Answer:\n",
      "Based on the provided information, Large Language Models (LLMs) are used in various sectors, including education, industry, and decision-making. \n",
      "\n",
      "According to the internal RAG result, LLMs are used in education [1], industry [2], and decision-making [3, 4]. This suggests that LLMs are versatile and can be applied in different fields to deliver precise and seamless interactions.\n",
      "\n",
      "In education, LLMs can be used to create personalized learning experiences, provide real-time feedback, and assist teachers with grading and lesson planning [1]. This is supported by research in the field of education technology, which highlights the potential of LLMs to improve student outcomes and enhance the learning experience.\n",
      "\n",
      "In industry, LLMs can be used to automate tasks, improve customer service, and enhance product development [2]. This is consistent with the growing trend of adopting AI and machine learning technologies in the industry to increase efficiency and productivity.\n",
      "\n",
      "In decision-making, LLMs can be used to analyze large amounts of data, identify patterns, and provide recommendations [3, 4]. This is supported by research in the field of artificial intelligence, which highlights the potential of LLMs to improve decision-making processes and reduce the risk of errors.\n",
      "\n",
      "Overall, the use of LLMs in various sectors highlights their potential to deliver precise and seamless interactions, improve efficiency and productivity, and enhance decision-making processes.\n",
      "\n",
      "References:\n",
      "[1] Internal RAG result (no specific reference number)\n",
      "[2] Internal RAG result (no specific reference number)\n",
      "[3] Internal RAG result (no specific reference number)\n",
      "[4] Internal RAG result (no specific reference number)\n",
      "[5] (Source: [Wikipedia]) Wikipedia does not have any specific information on the use of LLMs, but it does provide a general overview of the technology.\n",
      "[6] (Source: [Arxiv]) Arxiv does not have any specific information on the use of LLMs in the provided search results.\n",
      "\n",
      "Note: Since the internal RAG result does not provide specific reference numbers, I have assigned numbers [1], [2], [3], and [4] to the corresponding points. The Wikipedia and Arxiv results do not provide any specific information on the use of LLMs, but they do provide general information on the technology.\n",
      "\n",
      "--- Test Query 2: Requires external search (e.g., Wikipedia) ---\n",
      "\n",
      " Entering Internal KB Node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8d107333eb46cdb2912031085bb87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Deciding next step with Router LLM for question: Who developed LangChain and what is its primary us...\n",
      "  Internal RAG snippet: According to the provided context, LangChain was developed by Harrison Chase [Wikipedia]. \n",
      "\n",
      "As for i...\n",
      "  Router decision: wikipedia\n",
      "\n",
      " Entering Wikipedia Node\n",
      "\n",
      " Calling Wikipedia Search for: Who developed LangChain and what is its primary use?\n",
      " Wikipedia result: Content: Generation Z, often shortened to Gen Z and informally known as Zoomers, is the demographic ...\n",
      "\n",
      " Entering Final Answer Node\n",
      " Final Answer generated: Based on the provided information, I can answer the user's question about LangChain.\n",
      "\n",
      "**Who develope...\n",
      "Final Synthesized Answer:\n",
      "Based on the provided information, I can answer the user's question about LangChain.\n",
      "\n",
      "**Who developed LangChain?**\n",
      "LangChain was developed by Harrison Chase (Source: Wikipedia).\n",
      "\n",
      "**What is its primary use?**\n",
      "LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications, with use-cases including document analysis and summarization, chatbots, and code analysis (Source: Wikipedia).\n",
      "\n",
      "My reasoning for using specific information is as follows:\n",
      "\n",
      "* I used the Wikipedia result as the primary source for information about LangChain, as it is a reliable online encyclopedia that provides accurate and up-to-date information on various topics, including technology and software frameworks.\n",
      "* I did not use the Wikipedia result about Generation Z, as it is unrelated to the topic of LangChain and its primary use.\n",
      "* I did not use the Arxiv result, as there were no relevant results found.\n",
      "* I did not use the internal RAG result, as it only provided a single piece of information about Harrison Chase being the developer of LangChain, but did not provide any information about its primary use.\n",
      "\n",
      "Overall, the information about LangChain is synthesized from multiple sources, including Wikipedia, to provide a comprehensive answer to the user's question.\n",
      "\n",
      "--- Test Query 3: Requires external search (e.g., Arxiv for technical details) ---\n",
      "\n",
      " Entering Internal KB Node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be29b585a56d40a2b154872d5a8328cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Deciding next step with Router LLM for question: What is the transformer architecture and what are ...\n",
      "  Internal RAG snippet: Unfortunately, the provided context does not contain information about the transformer architecture....\n",
      "  Router decision: wikipedia\n",
      "\n",
      " Entering Wikipedia Node\n",
      "\n",
      " Calling Wikipedia Search for: What is the transformer architecture and what are its key components?\n",
      " Wikipedia result: Content: In deep learning, the transformer is an artificial neural network architecture based on the...\n",
      "\n",
      " Entering Final Answer Node\n",
      " Final Answer generated: The transformer architecture is a type of neural network architecture primarily used for natural lan...\n",
      "Final Synthesized Answer:\n",
      "The transformer architecture is a type of neural network architecture primarily used for natural language processing (NLP) tasks, such as language translation, text summarization, and question answering [1]. It was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017 [1].\n",
      "\n",
      "The key components of the transformer architecture include:\n",
      "\n",
      "1. **Self-Attention Mechanism**: This is a mechanism that allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. It is based on the idea of self-attention, which is a type of attention mechanism that allows the model to focus on different parts of the input sequence [1].\n",
      "2. **Encoder-Decoder Structure**: The transformer architecture consists of an encoder and a decoder. The encoder takes in the input sequence and produces a continuous representation of the input, while the decoder generates the output sequence [1].\n",
      "3. **Multi-Head Attention**: The transformer architecture uses a multi-head attention mechanism, which allows the model to attend to different parts of the input sequence in parallel and weigh their importance [1].\n",
      "4. **Positional Encoding**: The transformer architecture uses positional encoding to preserve the order of the input sequence. This is necessary because the self-attention mechanism does not take into account the order of the input sequence [1].\n",
      "\n",
      "According to Wikipedia, the transformer architecture is based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table [2]. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified [2].\n",
      "\n",
      "The transformer architecture has been widely used in various NLP tasks and has achieved state-of-the-art results in many benchmarks [1]. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models [2].\n",
      "\n",
      "References:\n",
      "[1] Vaswani et al. (2017). Attention is All You Need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS 2017).\n",
      "[2] Wikipedia. Transformer (machine learning).\n",
      "\n",
      "--- Test Query 4: More complex, potentially needing multiple tools ---\n",
      "\n",
      " Entering Internal KB Node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559bce70c604437f93ef4c8abe925118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Deciding next step with Router LLM for question: Tell me about the recent advancements in Large Lan...\n",
      "  Internal RAG snippet: Based on the provided context, I can tell you that there have been recent advancements in Large Lang...\n",
      "  Router decision: arxiv\n",
      "\n",
      " Entering Arxiv Node\n",
      "\n",
      " Calling Arxiv Search for: Tell me about the recent advancements in Large Language Models.\n",
      " Arxiv result: Content: Reinforcement Learning Meets Large Language Models: A Survey of\n",
      "Advancements and Applicatio...\n",
      "\n",
      " Entering Final Answer Node\n",
      " Final Answer generated: Recent advancements in Large Language Models (LLMs) have been significant, with applications across ...\n",
      "Final Synthesized Answer:\n",
      "Recent advancements in Large Language Models (LLMs) have been significant, with applications across various domains. According to a preprint on arXiv titled \"vision-language understanding with advanced large language models\" (arXiv:2304.10592), there have been advancements in LLMs for vision-language understanding. However, the content of the preprint is not provided.\n",
      "\n",
      "A more comprehensive overview of recent advancements in LLMs can be found in the arXiv preprint \"Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle\" (Source: [Arxiv]). This paper provides a survey of advancements and applications across the LLM lifecycle, highlighting the integration of reinforcement learning with LLMs.\n",
      "\n",
      "Additionally, recent research has focused on the limitations of LLMs, particularly their understanding of character composition of words. A study titled \"Large Language Models Lack Understanding of Character Composition of Words\" (arXiv:2405.11357v3) (Source: [Arxiv]) found that LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters.\n",
      "\n",
      "Furthermore, researchers have investigated the personality traits of LLMs, with a study titled \"Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality\" (Source: [Arxiv]) assessing the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies.\n",
      "\n",
      "In summary, recent advancements in LLMs have been significant, with applications in vision-language understanding, reinforcement learning, and character composition of words. However, LLMs still have limitations, particularly in understanding the minimal units of text, namely characters.\n",
      "\n",
      "References:\n",
      "[Arxiv] arXiv:2304.10592 (vision-language understanding with advanced large language models)\n",
      "[Arxiv] arXiv:2405.11357v3 (Large Language Models Lack Understanding of Character Composition of Words)\n",
      "[Arxiv] arXiv:2405.11357v3 (Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality)\n",
      "\n",
      "Reasoning:\n",
      "I used the arXiv preprints as primary sources of information, as they provide a comprehensive overview of recent advancements in LLMs. The preprints on vision-language understanding, reinforcement learning, and character composition of words provide a clear understanding of the current state of LLMs. I also considered the limitations of LLMs, as highlighted in the study on character composition of words. The study on personality traits of LLMs provides additional insights into the capabilities and limitations of LLMs.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 11. Final Answer Synthesis\n",
    "# -------------------------------\n",
    "final_answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an intelligent assistant. Synthesize the following information to provide a comprehensive answer to the user's question.\n",
    "               Clearly cite sources like (Source: [PDF]), (Source: [Web]), (Source: [Wikipedia]), or (Source: [Arxiv]) when information is used.\n",
    "               If a source isn't explicitly mentioned, state it as 'General knowledge' or 'Synthesized from multiple sources'.\n",
    "               If no relevant information is found across all sources, state that you cannot answer the question.\n",
    "               Explain your reasoning for using specific information.\n",
    "\n",
    "               Original Question: {question}\n",
    "               Internal RAG Result: {internal_rag_result}\n",
    "               Wikipedia Result: {external_wiki_result}\n",
    "               Arxiv Result: {external_arxiv_result}\"\"\"),\n",
    "    (\"human\", \"Generate final comprehensive answer with proper citations and explain your reasoning.\")\n",
    "])\n",
    "\n",
    "def final_answer_node(state: AgentState):\n",
    "    print(\"\\n Entering Final Answer Node\")\n",
    "    context = {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"internal_rag_result\": state.get(\"internal_rag_result\", \"No results from internal RAG.\"),\n",
    "        \"external_wiki_result\": state.get(\"external_wiki_result\", \"No results from Wikipedia.\"),\n",
    "        \"external_arxiv_result\": state.get(\"external_arxiv_result\", \"No results from Arxiv.\")\n",
    "    }\n",
    "    final_answer = (final_answer_prompt | llm | StrOutputParser()).invoke(context)\n",
    "    print(f\" Final Answer generated: {final_answer[:100]}...\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=final_answer)],\n",
    "        \"final_answer_text\": final_answer\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Router LLM\n",
    "# -------------------------------\n",
    "llm_router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a routing assistant. Based on the user's question and the initial internal RAG result, decide the next step.\n",
    "\n",
    "               Respond with ONLY ONE of these exact words (lowercase):\n",
    "               - synthesize (if internal RAG has sufficient information and no further search is needed)\n",
    "               - wikipedia (if you need general encyclopedia information to supplement or clarify)\n",
    "               - arxiv (if you need academic research papers to supplement or clarify)\n",
    "\n",
    "               Question: {question}\n",
    "               Internal RAG Result: {internal_rag_result}\"\"\"),\n",
    "    (\"human\", \"What should be the next step?\")\n",
    "])\n",
    "\n",
    "llm_router = llm_router_prompt | llm | StrOutputParser()\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"synthesize\",\"wikipedia_search\",\"arxiv_search\"]:\n",
    "    question = state[\"question\"]\n",
    "    internal_rag_result = state.get(\"internal_rag_result\",\"\")\n",
    "\n",
    "    print(f\"\\n Deciding next step with Router LLM for question: {question[:50]}...\")\n",
    "    print(f\"  Internal RAG snippet: {internal_rag_result[:100]}...\")\n",
    "\n",
    "    decision = llm_router.invoke({\n",
    "        \"question\": question,\n",
    "        \"internal_rag_result\": internal_rag_result\n",
    "    }).lower().strip()\n",
    "\n",
    "    print(f\"  Router decision: {decision}\")\n",
    "\n",
    "    if \"synthesize\" in decision:\n",
    "        return \"Final Answer\"\n",
    "    elif \"wikipedia\" in decision:\n",
    "        return \"Wikipedia\"\n",
    "    elif \"arxiv\" in decision:\n",
    "        return \"Arxiv\"\n",
    "    else:\n",
    "        # Fallback if LLM gives unexpected output (e.g., if it hallucinates a tool name)\n",
    "        print(\"  Router produced unexpected decision, defaulting to Wikipedia if RAG insufficient, else synthesize.\")\n",
    "        if \"i don't have enough information\" in internal_rag_result.lower() or \"no results found\" in internal_rag_result.lower():\n",
    "            return \"Wikipedia\"\n",
    "        return \"Final Answer\"\n",
    "\n",
    "# -------------------------------\n",
    "# 13. Build LangGraph (FIXED - removed color parameter)\n",
    "# -------------------------------\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"Internal KB\", rag_node)\n",
    "graph.add_node(\"Wikipedia\", wiki_node)\n",
    "graph.add_node(\"Arxiv\", arxiv_node)\n",
    "graph.add_node(\"Final Answer\", final_answer_node)\n",
    "\n",
    "graph.set_entry_point(\"Internal KB\")\n",
    "\n",
    "graph.add_conditional_edges(\"Internal KB\", should_continue, {\n",
    "    \"Final Answer\": \"Final Answer\",\n",
    "    \"Wikipedia\": \"Wikipedia\",\n",
    "    \"Arxiv\": \"Arxiv\"\n",
    "})\n",
    "\n",
    "graph.add_edge(\"Wikipedia\", \"Final Answer\")\n",
    "graph.add_edge(\"Arxiv\", \"Final Answer\")\n",
    "graph.add_edge(\"Final Answer\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "print(\" LangGraph agent compiled successfully!\")\n",
    "\n",
    "# --- Display the graph visual ---\n",
    "print(\"\\n--- LangGraph Visual ---\")\n",
    "# Ensure graphviz is installed for drawing, e.g., !apt-get install -y graphviz\n",
    "# If you encounter issues, this step might require additional troubleshooting for your environment.\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying graph visual: {e}\")\n",
    "    print(\"Please ensure 'graphviz' is installed (e.g., !apt-get install -y graphviz) and its dependencies are met.\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 14. Test Queries\n",
    "# -------------------------------\n",
    "print(\"\\n--- Test Query 1: Internal RAG should be sufficient ---\")\n",
    "query1 = \"What are Large Language Models used for?\"\n",
    "initial_state1 = {\"messages\": [HumanMessage(content=query1)], \"question\": query1}\n",
    "result1 = app.invoke(initial_state1)\n",
    "print(\"Final Synthesized Answer:\")\n",
    "print(result1[\"final_answer_text\"])\n",
    "\n",
    "print(\"\\n--- Test Query 2: Requires external search (e.g., Wikipedia) ---\")\n",
    "query2 = \"Who developed LangChain and what is its primary use?\"\n",
    "initial_state2 = {\"messages\": [HumanMessage(content=query2)], \"question\": query2}\n",
    "result2 = app.invoke(initial_state2)\n",
    "print(\"Final Synthesized Answer:\")\n",
    "print(result2[\"final_answer_text\"])\n",
    "\n",
    "print(\"\\n--- Test Query 3: Requires external search (e.g., Arxiv for technical details) ---\")\n",
    "query3 = \"What is the transformer architecture and what are its key components?\"\n",
    "initial_state3 = {\"messages\": [HumanMessage(content=query3)], \"question\": query3}\n",
    "result3 = app.invoke(initial_state3)\n",
    "print(\"Final Synthesized Answer:\")\n",
    "print(result3[\"final_answer_text\"])\n",
    "\n",
    "print(\"\\n--- Test Query 4: More complex, potentially needing multiple tools ---\")\n",
    "query4 = \"Tell me about the recent advancements in Large Language Models.\"\n",
    "initial_state4 = {\"messages\": [HumanMessage(content=query4)], \"question\": query4}\n",
    "result4 = app.invoke(initial_state4)\n",
    "print(\"Final Synthesized Answer:\")\n",
    "print(result4[\"final_answer_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9sc3GiY8G4m"
   },
   "source": [
    "# **Text Summarizer RAG Pipeline – Workflow Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmTkU94W8FFW"
   },
   "source": [
    "## Text Summarizer RAG Pipeline – Workflow Overview\n",
    "\n",
    "The text summarizer RAG pipeline, as implemented in the provided code, follows these main steps:\n",
    "\n",
    "### 1. Load Sources\n",
    "- Documents from various sources (PDF, Web, Wikipedia, Arxiv) are loaded.\n",
    "- For the summarizer specifically, text content is loaded from local PDF files:\n",
    "  - `/content/LLM.pdf`\n",
    "  - `/content/apjspeech.pdf`\n",
    "- PDF loading is performed using **PyMuPDF (`fitz`)**.\n",
    "\n",
    "### 2. Combine and Truncate Content\n",
    "- Content from all loaded sources is combined into a single string.\n",
    "- To manage LLM token limits, the combined content is truncated to a maximum length (e.g., **20,000 characters**).\n",
    "\n",
    "### 3. LLM Setup\n",
    "- A **ChatGroq** model (`llama-3.1-8b-instant`) is initialized.\n",
    "- Key parameters include:\n",
    "  - `temperature`\n",
    "  - `max_tokens` (increased to **1000** for more comprehensive summaries)\n",
    "\n",
    "### 4. Summarization Prompt\n",
    "- A `ChatPromptTemplate` is defined to guide the summarization process.\n",
    "- It includes:\n",
    "  - A **system instruction** for expert-level summarization\n",
    "  - A **human input template** that injects the combined document content into the prompt\n",
    "\n",
    "### 5. Summarization Chain\n",
    "- A `Runnable` chain is created by piping:\n",
    "  - `summarization_prompt`\n",
    "  - `llm`\n",
    "  - `StrOutputParser`\n",
    "- This chain:\n",
    "  - Accepts document content as input\n",
    "  - Formats it using the prompt\n",
    "  - Sends it to the LLM for summarization\n",
    "  - Parses the LLM output into a string\n",
    "\n",
    "### 6. Invoke and Display Summary\n",
    "- The summarization chain is invoked with the truncated combined content.\n",
    "- The generated summary is printed as the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrfjAwRN5Gg3",
    "outputId": "3a35ba07-2a08-415d-bd79-8f0c02437874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded content from /content/LLM.pdf\n",
      "Successfully loaded content from /content/apjspeech.pdf\n",
      "Truncated combined_content to 20000 characters.\n",
      "\n",
      "--- Generating Summary of Local Knowledge Base ---\n",
      "\n",
      "--- Summary of PDF Documents ---\n",
      "**Comprehensive Overview of Large Language Models**\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "Large Language Models (LLMs) have revolutionized natural language processing (NLP) tasks, demonstrating remarkable capabilities in text generation, understanding, and reasoning. This article provides a comprehensive overview of the recent developments in LLM research, covering various aspects, including architectures, training strategies, fine-tuning, and applications.\n",
      "\n",
      "**Background**\n",
      "\n",
      "LLMs are built upon the transformer architecture, which processes input sequences in parallel and independently. Tokenization, encoding positions, attention, and activation functions are essential components of LLMs. Tokenization involves parsing text into non-decomposing units called tokens, while encoding positions use positional embeddings to capture the order of tokens. Attention assigns weights to input tokens based on importance, and activation functions, such as ReLU and GeLU, help the model learn complex relationships between tokens.\n",
      "\n",
      "**LLM Architectures**\n",
      "\n",
      "LLMs employ various transformer architectures, including:\n",
      "\n",
      "1. **Encoder-Decoder**: Processes inputs through the encoder and passes the intermediate representation to the decoder to generate the output.\n",
      "2. **Causal Decoder**: Processes and generates output using a decoder, where the predicted token depends only on the previous time steps.\n",
      "3. **Prefix Decoder**: Also known as a non-causal decoder, where the attention calculation is not strictly dependent on the past information and the attention is bidirectional.\n",
      "4. **Mixture-of-Experts**: A variant of transformer architecture with parallel independent experts and a router to route tokens to experts.\n",
      "\n",
      "**Training Strategies**\n",
      "\n",
      "LLMs are trained using various strategies, including:\n",
      "\n",
      "1. **Pre-training**: Trains the model on a large corpus of text to learn a generic representation that is shareable among various NLP tasks.\n",
      "2. **Fine-tuning**: Trains the model on a specific task or dataset to adapt to the task requirements.\n",
      "3. **Distributed Training**: Uses multiple devices to train the model in parallel, reducing training time and increasing model capacity.\n",
      "\n",
      "**Efficient LLM Utilization**\n",
      "\n",
      "To improve the efficiency of LLMs, researchers have proposed various techniques, including:\n",
      "\n",
      "1. **Parameter Efficient Tuning**: Reduces the number of parameters in the model while maintaining its performance.\n",
      "2. **Pruning**: Removes unnecessary parameters from the model to reduce its size and computational requirements.\n",
      "3. **Quantization**: Reduces the precision of model weights and activations to reduce memory requirements.\n",
      "4. **Knowledge Distillation**: Transfers knowledge from a large model to a smaller model to improve its performance.\n",
      "\n",
      "**Applications and Challenges**\n",
      "\n",
      "LLMs have been applied in various domains, including:\n",
      "\n",
      "1. **Natural Language Processing**: Text classification, sentiment analysis, machine translation, and question answering.\n",
      "2. **Multi-modal Processing**: Integrating text with images, audio, and other modalities.\n",
      "3. **Robotics**: Using LLMs to control robots and perform tasks.\n",
      "4. **Autonomous Agents**: Using LLMs to make decisions and take actions in complex environments.\n",
      "\n",
      "However, LLMs also face several challenges, including:\n",
      "\n",
      "1. **Explainability**: Understanding how LLMs make decisions and generate text.\n",
      "2. **Adversarial Attacks**: Defending LLMs against malicious inputs designed to mislead or deceive the model.\n",
      "3. **Bias and Fairness**: Ensuring that LLMs do not perpetuate biases and stereotypes present in the training data.\n",
      "4. **Scalability**: Scaling LLMs to larger models and datasets while maintaining their performance and efficiency.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "This article provides a comprehensive overview of the recent developments in LLM research, covering various aspects, including architectures, training strategies, fine-tuning, and applications. While LLMs have demonstrated remarkable capabilities in NLP tasks, they also face several challenges that need to be addressed to ensure their safe and effective deployment in various domains.\n"
     ]
    }
   ],
   "source": [
    "import fitz # PyMuPDF for handling PDF files\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define a helper function to load text content from a PDF file using PyMuPDF\n",
    "def load_pdf_content(file_path: str) -> str:\n",
    "    \"\"\"Loads text content from a PDF file using PyMuPDF.\"\"\"\n",
    "    text_content = \"\"\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            text_content += page.get_text()\n",
    "        doc.close()\n",
    "        print(f\"Successfully loaded content from {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PDF file not found at {file_path}. Skipping this file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {file_path}: {e}\")\n",
    "    return text_content\n",
    "\n",
    "# Paths to the PDF files specified by the user\n",
    "pdf_path_llm = \"/content/LLM.pdf\"\n",
    "pdf_path_apjspeech = \"/content/apjspeech.pdf\"\n",
    "\n",
    "# Load content from both PDFs\n",
    "llm_content = load_pdf_content(pdf_path_llm)\n",
    "apjspeech_content = load_pdf_content(pdf_path_apjspeech)\n",
    "\n",
    "# Combine the content for summarization\n",
    "combined_content = \"\"\n",
    "if llm_content:\n",
    "    combined_content += f\"--- Content from {pdf_path_llm} ---\\n{llm_content}\\n\\n\"\n",
    "if apjspeech_content:\n",
    "    combined_content += f\"--- Content from {pdf_path_apjspeech} ---\\n{apjspeech_content}\\n\\n\"\n",
    "\n",
    "# Truncate combined_content to a manageable size (e.g., 20000 characters)\n",
    "# to avoid exceeding model token limits.\n",
    "MAX_CONTENT_LENGTH = 20000\n",
    "if len(combined_content) > MAX_CONTENT_LENGTH:\n",
    "    combined_content = combined_content[:MAX_CONTENT_LENGTH]\n",
    "    print(f\"Truncated combined_content to {MAX_CONTENT_LENGTH} characters.\")\n",
    "\n",
    "if not combined_content:\n",
    "    print(\"No readable content found from the specified PDF files. Cannot summarize.\")\n",
    "else:\n",
    "    # Define a summarization prompt\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert summarizer. Summarize the following document(s) concisely and informatively. Focus on key concepts and main points.\"),\n",
    "        (\"human\", \"Document(s) to summarize:\\n\\n{document_content}\\n\\nProvide a comprehensive summary:\")\n",
    "    ])\n",
    "\n",
    "    # Create a summarization chain using the existing LLM\n",
    "    # Note: The 'llm' is configured with max_tokens=300, which might result in short summaries\n",
    "    # for very long documents. Consider adjusting llm.max_tokens if a longer summary is desired.\n",
    "    summarization_chain = summarization_prompt | llm | StrOutputParser()\n",
    "\n",
    "    print(\"\\n--- Generating Summary of Local Knowledge Base ---\")\n",
    "    summary = summarization_chain.invoke({\"document_content\": combined_content})\n",
    "\n",
    "    print(\"\\n--- Summary of PDF Documents ---\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0db31401",
    "outputId": "48f3ddc2-f6a1-4839-a15b-347c85816c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Follow-up Question: The summary briefly touches upon challenges like 'Explainability' and 'Adversarial Attacks' in LLMs. Could you elaborate on these specific challenges and how researchers are currently trying to mitigate them?\n",
      "\n",
      "Summarized Follow-up Question: Researchers are currently addressing the challenges of \"Explainability\" and \"Adversarial Attacks\" in Large Language Models (LLMs) by developing techniques such as model interpretability methods, adversarial training, and robustness testing to improve transparency and security.\n"
     ]
    }
   ],
   "source": [
    "# New follow-up question based on the summary\n",
    "new_question = \"The summary briefly touches upon challenges like 'Explainability' and 'Adversarial Attacks' in LLMs. Could you elaborate on these specific challenges and how researchers are currently trying to mitigate them?\"\n",
    "\n",
    "# Summarize the new question using the LLM\n",
    "question_summarization_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise summarizer. Summarize the following question in a single sentence.\"),\n",
    "    (\"human\", \"{question_text}\")\n",
    "])\n",
    "\n",
    "summarized_question_chain = question_summarization_prompt | llm | StrOutputParser()\n",
    "\n",
    "summarized_new_question = summarized_question_chain.invoke({\"question_text\": new_question})\n",
    "\n",
    "print(\"Original Follow-up Question:\", new_question)\n",
    "print(\"\\nSummarized Follow-up Question:\", summarized_new_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2rTskj88LWf",
    "outputId": "a3864555-bf5c-47f0-eb15-e34dd8b1d6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing summarizer_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summarizer_app.py\n",
    "\n",
    "import os\n",
    "import fitz # PyMuPDF for handling PDF files\n",
    "\n",
    "from google.colab import userdata\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- API Key Setup ---\n",
    "# Ensure GROQ_API_KEY is fetched directly from Colab secrets or environment\n",
    "# If running outside Colab, ensure os.environ[\"GROQ_API_KEY\"] is set\n",
    "api_key = userdata.get(\"GROQ_API_KEY\")\n",
    "\n",
    "# If the API key is still not found, raise an error or inform the user\n",
    "if not api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not found. Please ensure it is set as a Colab secret or environment variable.\")\n",
    "os.environ[\"GROQ_API_KEY\"] = api_key\n",
    "\n",
    "# --- LLM Setup ---\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# --- PDF Content Loader Function ---\n",
    "def load_pdf_content(file_path: str) -> str:\n",
    "    \"\"\"Loads text content from a PDF file using PyMuPDF.\"\"\"\n",
    "    text_content = \"\"\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            text_content += page.get_text()\n",
    "        doc.close()\n",
    "        print(f\"Successfully loaded content from {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: PDF file not found at {file_path}. Skipping this file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {file_path}: {e}\")\n",
    "    return text_content\n",
    "\n",
    "# --- Paths to PDF Files ---\n",
    "pdf_path_llm = \"/content/LLM.pdf\"\n",
    "pdf_path_apjspeech = \"/content/apjspeech.pdf\"\n",
    "\n",
    "# --- Load and Combine Content ---\n",
    "llm_content = load_pdf_content(pdf_path_llm)\n",
    "apjspeech_content = load_pdf_content(pdf_path_apjspeech)\n",
    "\n",
    "combined_content = \"\"\n",
    "if llm_content:\n",
    "    combined_content += f\"--- Content from {pdf_path_llm} ---\\n{llm_content}\\n\\n\"\n",
    "if apjspeech_content:\n",
    "    combined_content += f\"--- Content from {pdf_path_apjspeech} ---\\n{apjspeech_content}\\n\\n\"\n",
    "\n",
    "# Truncate combined_content to avoid exceeding model token limits\n",
    "MAX_CONTENT_LENGTH = 20000\n",
    "if len(combined_content) > MAX_CONTENT_LENGTH:\n",
    "    combined_content = combined_content[:MAX_CONTENT_LENGTH]\n",
    "    print(f\"Truncated combined_content to {MAX_CONTENT_LENGTH} characters.\")\n",
    "\n",
    "# --- Summarization Logic ---\n",
    "if not combined_content:\n",
    "    print(\"No readable content found from the specified PDF files. Cannot summarize.\")\n",
    "else:\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert summarizer. Summarize the following document(s) concisely and informatively. Focus on key concepts and main points.\"),\n",
    "        (\"human\", \"Document(s) to summarize:\\n\\n{document_content}\\n\\nProvide a comprehensive summary:\")\n",
    "    ])\n",
    "\n",
    "    summarization_chain = summarization_prompt | llm | StrOutputParser()\n",
    "\n",
    "    print(\"\\n--- Generating Summary of Local Knowledge Base ---\")\n",
    "    summary = summarization_chain.invoke({\"document_content\": combined_content})\n",
    "\n",
    "    print(\"\\n--- Summary of PDF Documents ---\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzHihaVI8sPa"
   },
   "source": [
    "## LangChain RAG Pipeline and Text Summarization Project – Overview\n",
    "\n",
    "This project demonstrates the construction and application of a versatile **LangChain-based Retrieval-Augmented Generation (RAG) pipeline** alongside a dedicated **text summarization tool**.\n",
    "\n",
    "### 1. RAG Agent Functionality\n",
    "- Implements a dynamic RAG agent capable of intelligent query routing.\n",
    "- Queries are directed to:\n",
    "  - An internal knowledge base\n",
    "  - Wikipedia\n",
    "  - Arxiv\n",
    "- Ensures comprehensive, accurate responses with proper **source citations**.\n",
    "\n",
    "### 2. Modular Architecture\n",
    "- Uses **HuggingFaceEmbeddings** and **FAISS** for efficient document embedding and retrieval.\n",
    "- Integrates **ChatGroq** for:\n",
    "  - Intelligent query routing\n",
    "  - Response generation\n",
    "- The modular design enables adaptive and scalable information retrieval.\n",
    "\n",
    "### 3. PDF Text Summarization Component\n",
    "- Implements a robust PDF summarizer within the same notebook.\n",
    "- Extracts text from local PDF files using **PyMuPDF**.\n",
    "- Combines extracted content into a single text corpus.\n",
    "- Generates concise, informative summaries using the **ChatGroq LLM**.\n",
    "\n",
    "### 4. Token Management and Reliability\n",
    "- Applies content truncation to manage LLM token limits.\n",
    "- Prevents `APIStatusError` when processing large documents.\n",
    "- Ensures stable and reliable summarization performance.\n",
    "\n",
    "### 5. Overall System Capabilities\n",
    "- Supports:\n",
    "  - In-depth, citation-backed question answering\n",
    "  - Efficient content distillation through summarization\n",
    "- Highlights the flexibility and power of **LangChain** for building advanced, production-ready LLM applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVbvVsLzMwYS"
   },
   "source": [
    "# **LangChain RAG_YouTube_Summarizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUgyDbPyBvZw",
    "outputId": "5e70a092-855e-4fd1-8011-a72e90ccebb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping youtube-transcript-api as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting youtube-transcript-api\n",
      "  Downloading youtube_transcript_api-1.2.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2026.1.4)\n",
      "Downloading youtube_transcript_api-1.2.3-py3-none-any.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.1/485.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
      "Successfully installed youtube-transcript-api-1.2.3\n"
     ]
    }
   ],
   "source": [
    "# First, reinstall the correct version\n",
    "!pip uninstall -y youtube-transcript-api\n",
    "!pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnEpTN8cFmCA",
    "outputId": "ee37a4c7-7186-483c-e5ec-1b51ca77a116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Import successful\n",
      "Available attributes: ['fetch', 'list']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Completely remove and reinstall\n",
    "\n",
    "# Step 2: Restart runtime or reimport\n",
    "import importlib\n",
    "import sys\n",
    "if 'youtube_transcript_api' in sys.modules:\n",
    "    del sys.modules['youtube_transcript_api']\n",
    "\n",
    "# Step 3: Fresh import\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Step 4: Verify installation\n",
    "print(f\" Import successful\")\n",
    "print(f\"Available attributes: {[attr for attr in dir(YouTubeTranscriptApi) if not attr.startswith('_')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU8rqFQ2HTUY"
   },
   "source": [
    "# **Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJT0e1JjFq7W",
    "outputId": "9b5e0f74-0dca-43ee-8cc8-7965b90a4dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import OK\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'fetch', 'list']\n",
      " Captions found in page\n",
      "Captions available but need proper parsing\n"
     ]
    }
   ],
   "source": [
    "# Manual installation check\n",
    "!python -c \"from youtube_transcript_api import YouTubeTranscriptApi; print('Import OK'); print(dir(YouTubeTranscriptApi))\"\n",
    "\n",
    "# If the above shows the import works but no methods, try this workaround:\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def get_transcript_manual(video_id: str) -> str:\n",
    "    \"\"\"Manual transcript extraction as fallback.\"\"\"\n",
    "    try:\n",
    "        # This uses the YouTube API endpoint directly\n",
    "        url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Extract captions URL from page source (simplified)\n",
    "        # Note: This is a basic approach and may not always work\n",
    "        if \"captionTracks\" in response.text:\n",
    "            print(\" Captions found in page\")\n",
    "            # For a more robust solution, you'd need to parse the JSON\n",
    "            # This is just to verify the video has captions\n",
    "            return \"Captions available but need proper parsing\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\" Manual method error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Test manual method\n",
    "result = get_transcript_manual(\"AOQyRiwydyo\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6vYwBmrqGnBB",
    "outputId": "63356f7d-93e3-4628-b5da-06762bf64b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is a promotional video script for a LangChain course, which is a framework for building AI orchestration pipelines. The speaker aims to teach data engineers how to learn LangChain and become proficient in building AI agents.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "1. **Why learn LangChain?** LangChain is a popular and in-demand framework, and knowing it can give data engineers an edge in the job market.\n",
      "2. **Prerequisites:** Basic Python understanding is required, including functions, loops, conditions, and basic object-oriented programming (OOP) concepts.\n",
      "3. **Course Overview:** The course will cover LangChain from scratch, with dedicated chapters, notebooks, and sessions. The speaker will provide live illustrations and notes, and all code examples will be available.\n",
      "4. **What is LangChain?** LangChain is a framework for building AI agents, which are essentially software programs that perform specific tasks. AI agents are created by integrating tools with Large Language Models (LLMs).\n",
      "5. **What is an AI Agent?** An AI agent is a software program that performs a specific task, similar to a real human. It is created by integrating tools with LLMs.\n",
      "6. **Why create tools?** Tools are created to extend the capabilities of LLMs, allowing them to perform tasks that they were not originally designed for. This is demonstrated by the example of ChatGPT, which was able to answer a question about the temperature in Canada by integrating a tool to access real-time data.\n",
      "7. **The backbone of AI agents:** LLMs with tools are the backbone of AI agents. Tools are functions that are integrated with LLMs to create autonomous agents.\n",
      "\n",
      "**Main Takeaways:**\n",
      "\n",
      "1. LangChain is a popular and in-demand framework for building AI orchestration pipelines.\n",
      "2. Basic Python understanding is required to learn LangChain.\n",
      "3. LangChain is a framework for building AI agents, which are created by integrating tools with LLMs.\n",
      "4. Tools are created to extend the capabilities of LLMs, allowing them to perform tasks that they were not originally designed for.\n",
      "5. LLMs with tools are the backbone of AI agents.\n"
     ]
    }
   ],
   "source": [
    "# Import API to fetch YouTube video transcripts\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Import prompt template for structured LLM prompting\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import output parser to convert LLM output to string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Define the summarization prompt with system and human roles\n",
    "# The system message sets the behavior of the LLM as an expert summarizer\n",
    "# The human message injects the document content dynamically\n",
    "summarization_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an expert summarizer. Summarize the following document(s) concisely and informatively. Focus on key concepts and main points.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"Document(s) to summarize:\\n\\n{document_content}\\n\\nProvide a comprehensive summary:\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Create a summarization chain by connecting the prompt, LLM, and output parser\n",
    "summarization_chain = summarization_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def get_youtube_transcript(video_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches and returns the transcript text of a YouTube video\n",
    "    given its URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract the YouTube video ID from standard or short URLs\n",
    "        if \"v=\" in video_url:\n",
    "            video_id = video_url.split(\"v=\")[1].split(\"&\")[0]\n",
    "        elif \"youtu.be/\" in video_url:\n",
    "            video_id = video_url.split(\"youtu.be/\")[1].split(\"?\")[0]\n",
    "        else:\n",
    "            # Raise an error if the URL format is not recognized\n",
    "            raise ValueError(\"Invalid YouTube URL format\")\n",
    "\n",
    "        # Initialize the YouTube Transcript API\n",
    "        api = YouTubeTranscriptApi()\n",
    "\n",
    "        # Fetch the transcript object using the video ID\n",
    "        transcript_obj = api.fetch(video_id)\n",
    "\n",
    "        # Handle different transcript object structures\n",
    "        if hasattr(transcript_obj, 'snippets'):\n",
    "            # If transcript contains snippets, extract text from each snippet\n",
    "            snippets = transcript_obj.snippets\n",
    "            transcript = \" \".join([snippet.text for snippet in snippets])\n",
    "        elif hasattr(transcript_obj, '__iter__'):\n",
    "            # If transcript is iterable, extract text from each item\n",
    "            transcript = \" \".join(\n",
    "                [getattr(item, 'text', str(item)) for item in transcript_obj]\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: convert the transcript object directly to string\n",
    "            transcript = str(transcript_obj)\n",
    "\n",
    "        # Return the extracted transcript text\n",
    "        return transcript\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors during transcript fetching or processing\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# YouTube video URL to be summarized\n",
    "youtube_video_url = \"https://www.youtube.com/watch?v=AOQyRiwydyo&t=6323s\"\n",
    "\n",
    "# Fetch the transcript for the given YouTube video\n",
    "video_transcript = get_youtube_transcript(youtube_video_url)\n",
    "\n",
    "# Proceed only if transcript extraction was successful\n",
    "if video_transcript:\n",
    "    # Define maximum allowed transcript length to control token usage\n",
    "    MAX_TRANSCRIPT_LENGTH = 20000\n",
    "    original_length = len(video_transcript)\n",
    "\n",
    "    # Truncate transcript if it exceeds the maximum length\n",
    "    if len(video_transcript) > MAX_TRANSCRIPT_LENGTH:\n",
    "        video_transcript = video_transcript[:MAX_TRANSCRIPT_LENGTH]\n",
    "\n",
    "    # Generate a summary using the summarization chain\n",
    "    try:\n",
    "        youtube_summary = summarization_chain.invoke(\n",
    "            {\"document_content\": video_transcript}\n",
    "        )\n",
    "\n",
    "        # Print the generated summary\n",
    "        print(youtube_summary)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle errors during summary generation\n",
    "        print(\"Error generating summary:\", e)\n",
    "else:\n",
    "    # Handle the case where transcript could not be retrieved\n",
    "    print(\"Could not retrieve YouTube transcript\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfEzNf7kMJh4"
   },
   "source": [
    "## LangChain YouTube Summarizer – Comprehensive Overview\n",
    "\n",
    "This notebook’s YouTube summarizer provides a robust solution for extracting and summarizing content from YouTube videos using a **LangChain-based architecture**. Its core functionality includes the following components:\n",
    "\n",
    "### Transcript Extraction\n",
    "- Utilizes the `youtube-transcript-api` library to reliably fetch the full textual transcript from a given YouTube video URL.\n",
    "- Handles video ID parsing and includes error handling to manage transcript retrieval failures.\n",
    "\n",
    "### LLM Integration\n",
    "- Integrates seamlessly with a pre-configured **ChatGroq LLM** (`llama-3.1-8b-instant`).\n",
    "- Uses an increased `max_tokens` setting to enable more detailed and comprehensive summaries.\n",
    "\n",
    "### Context Management\n",
    "- Implements transcript truncation to ensure compatibility with LLM token limits.\n",
    "- Limits raw transcript content to a predefined `MAX_TRANSCRIPT_LENGTH` (for example, 20,000 characters) before passing it to the model.\n",
    "\n",
    "### Prompt Engineering\n",
    "- Uses a `ChatPromptTemplate` to guide the summarization process.\n",
    "- Instructs the LLM to behave as an expert summarizer, emphasizing concise, informative output focused on key concepts.\n",
    "\n",
    "### Summarization Chain\n",
    "- Builds a `Runnable` chain combining:\n",
    "  - The summarization prompt\n",
    "  - The LLM\n",
    "  - A `StrOutputParser`\n",
    "- Creates a streamlined and efficient workflow for generating the final summary.\n",
    "\n",
    "### Overall Purpose\n",
    "- Automates the conversion of lengthy YouTube video content into clear, digestible summaries.\n",
    "- Enables rapid understanding of video content while intelligently managing the constraints of large language models.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
